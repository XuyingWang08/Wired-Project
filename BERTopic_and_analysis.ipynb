{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lRgJvwghX7RX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bertopic[visualization]"
      ],
      "metadata": {
        "id": "6csoExlcYOlZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bertopic umap-learn hdbscan sentence-transformers"
      ],
      "metadata": {
        "id": "pOILDIitX-S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bertopic[visualization] umap-learn hdbscan sentence-transformers gensim nltk textblob networkx matplotlib scikit-learn"
      ],
      "metadata": {
        "id": "bs-EHPkCYB7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#spacy\n",
        "\n",
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_trf\n",
        "\n",
        "import spacy\n",
        "from spacy.pipeline import EntityRuler\n"
      ],
      "metadata": {
        "id": "zF9DKSVzX4fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate token statistics\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "# Read data\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/wired_project/wired_2014_2024_cleaned.csv\")\n",
        "texts = df[\"text\"].dropna().tolist()\n",
        "\n",
        "# Compute token length and truncation ratio\n",
        "max_len = 512\n",
        "token_lens = []\n",
        "percent_losses = []\n",
        "\n",
        "for text in texts:\n",
        "    tokens = tokenizer.encode(text, truncation=False)\n",
        "    total_len = len(tokens)\n",
        "    truncated_len = min(total_len, max_len)\n",
        "    loss = max(total_len - max_len, 0)\n",
        "    percent_loss = (loss / total_len) * 100 if total_len > 0 else 0\n",
        "\n",
        "    token_lens.append(total_len)\n",
        "    percent_losses.append(percent_loss)\n",
        "\n",
        "# Visualize distribution of truncation percentage\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.hist(percent_losses, bins=40, color=\"skyblue\", edgecolor=\"black\")\n",
        "plt.xlabel(\"Percentage Loss\")\n",
        "plt.ylabel(\"Number of Documents\")\n",
        "plt.title(\"Distribution of Percentage Loss by BGE Tokenizer (max 512 tokens)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Output basic statistics\n",
        "percent_losses_np = np.array(percent_losses)\n",
        "print(f\"Average truncation ratio: {percent_losses_np.mean():.2f}%\")\n",
        "print(f\"Median truncation ratio: {np.median(percent_losses_np):.2f}%\")\n",
        "print(f\"Number of documents with 0% truncation: {(percent_losses_np == 0).sum()} / {len(percent_losses_np)}\")\n",
        "print(f\"Number of documents with truncation ratio > 60%: {(percent_losses_np > 60).sum()} / {len(percent_losses_np)}\")\n"
      ],
      "metadata": {
        "id": "im6Y9VpzX5Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Config\n",
        "CSV_IN       = \"wired_2014_2024_cleaned.csv\"\n",
        "TEXT_COL     = \"text\"\n",
        "MODEL_NAME   = \"BAAI/bge-base-en-v1.5\"\n",
        "MAX_LEN      = 512\n",
        "BATCH_SIZE   = 64\n",
        "OUT_MEMMAP   = \"wired_bge-base-v1.5_len512.memmap\"\n",
        "NORMALIZE    = True\n",
        "\n",
        "print(f\"Loading data: {CSV_IN}\")\n",
        "df = pd.read_csv(CSV_IN)\n",
        "texts = df[TEXT_COL].astype(str).tolist()\n",
        "N = len(texts)\n",
        "print(f\"Articles: {N}\")\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "dummy = model.encode([\"test\"], normalize_embeddings=NORMALIZE, truncation=True, max_length=32)\n",
        "DIM = dummy.shape[1]\n",
        "print(f\"Embedding dim: {DIM}\")\n",
        "\n",
        "if os.path.exists(OUT_MEMMAP):\n",
        "    emb = np.memmap(OUT_MEMMAP, dtype=\"float16\", mode=\"r+\", shape=(N, DIM))\n",
        "    zero_rows = np.where(np.all(emb == 0, axis=1))[0]\n",
        "    pos = int(zero_rows[0]) if len(zero_rows) > 0 else N\n",
        "    print(f\"Resuming from position {pos}/{N}\")\n",
        "else:\n",
        "    emb = np.memmap(OUT_MEMMAP, dtype=\"float16\", mode=\"w+\", shape=(N, DIM))\n",
        "    pos = 0\n",
        "    print(\"Starting fresh run...\")\n",
        "\n",
        "# Main loop\n",
        "start_time = time.time()\n",
        "for i in tqdm(range(pos, N, BATCH_SIZE), desc=\"Generating embeddings\", unit=\"batch\"):\n",
        "    batch = texts[i:i+BATCH_SIZE]\n",
        "    vec = model.encode(\n",
        "        batch,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        normalize_embeddings=NORMALIZE,\n",
        "        truncation=True,\n",
        "        max_length=MAX_LEN,\n",
        "        show_progress_bar=False\n",
        "    ).astype(np.float16)\n",
        "    emb[i:i+len(batch)] = vec\n",
        "    emb.flush()\n",
        "\n",
        "    elapsed = time.time() - start_time\n",
        "    done = i + len(batch) - pos\n",
        "    total = N - pos\n",
        "    speed = done / elapsed\n",
        "    eta = (total - done) / speed if speed > 0 else 0\n",
        "    tqdm.write(f\"Batch {i//BATCH_SIZE+1}, speed={speed:.2f} docs/s, ETA={eta/60:.1f} min\")\n",
        "\n",
        "emb.flush()\n",
        "print(f\"Saved memmap: {OUT_MEMMAP}  shape=({N}, {DIM})  dtype=float16\")\n",
        "\n",
        "# Save metadata\n",
        "meta = {\n",
        "    \"model\": MODEL_NAME,\n",
        "    \"max_length\": MAX_LEN,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"normalize\": NORMALIZE,\n",
        "    \"n_articles\": N,\n",
        "    \"dim\": int(DIM)\n",
        "}\n",
        "pd.Series(meta).to_json(\"embedding_meta.json\", indent=2)\n",
        "print(\"Wrote embedding_meta.json\")\n"
      ],
      "metadata": {
        "id": "4LQGOVXkxb_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load article texts\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs_all = df[\"text\"].dropna().astype(str).tolist()\n",
        "N_TOTAL = len(docs_all)\n",
        "EMB_DIM = 768\n",
        "\n",
        "print(f\"Total documents: {N_TOTAL:,}\")\n",
        "\n",
        "#Delete old file if exists\n",
        "if os.path.exists(EMBED_PATH):\n",
        "    os.remove(EMBED_PATH)\n",
        "    print(\"Old memmap file deleted\")\n",
        "\n",
        "#Create memmap file and write embeddings\n",
        "print(\"Loading BGE model...\")\n",
        "model = SentenceTransformer(\"BAAI/bge-base-en-v1.5\")\n",
        "\n",
        "print(\"Generating embeddings and saving as memmap...\")\n",
        "batch_size = 32\n",
        "embeddings = np.memmap(EMBED_PATH, dtype=np.float32, mode='w+', shape=(N_TOTAL, EMB_DIM))\n",
        "\n",
        "for i in tqdm(range(0, N_TOTAL, batch_size)):\n",
        "    batch = docs_all[i:i+batch_size]\n",
        "    batch_emb = model.encode(batch, show_progress_bar=False)\n",
        "    embeddings[i:i+len(batch_emb)] = batch_emb\n",
        "\n",
        "embeddings.flush()\n",
        "print(f\"Memmap file saved as: {EMBED_PATH}\")\n",
        "print(f\"File shape: {embeddings.shape}\")"
      ],
      "metadata": {
        "id": "UoiuRYh0Irsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CSV_PATH = \"/content/drive/MyDrive/wired_project/wired_2014_2024_cleaned.csv\"\n",
        "EMBED_PATH = \"/content/drive/MyDrive/wired_project/wired_bge-base-v1.5_len512.memmap\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/wired_project\"\n",
        "DIM = 768\n",
        "\n",
        "final_params = {\n",
        "    \"min_cluster_size\": 100,\n",
        "    \"min_samples\": 15,\n",
        "    \"n_neighbors\": 12,\n",
        "    \"min_dist\": 0.02,\n",
        "    \"n_components\": 5,\n",
        "    \"nr_topics\": \"auto\",\n",
        "    \"top_n_words\": 10,\n",
        "    \"compute_npmi\": True,\n",
        "    \"npmi_docs\": 2000\n",
        "}\n",
        "\n",
        "# Load data and embeddings\n",
        "print(\"Loading texts...\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs_all = df[\"text\"].dropna().astype(str).tolist()\n",
        "print(f\"Number of documents: {len(docs_all):,}\")\n",
        "\n",
        "print(\"Loading embeddings...\")\n",
        "emb_all = np.memmap(EMBED_PATH, dtype=np.float32, mode=\"r\", shape=(len(docs_all), DIM))\n",
        "print(f\"Embedding matrix shape: {emb_all.shape}\")\n",
        "\n",
        "print(\"\\nTraining BERTopic model...\")\n",
        "\n",
        "umap_model = UMAP(\n",
        "    n_neighbors=final_params[\"n_neighbors\"],\n",
        "    n_components=final_params[\"n_components\"],\n",
        "    min_dist=final_params[\"min_dist\"],\n",
        "    metric=\"cosine\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=final_params[\"min_cluster_size\"],\n",
        "    min_samples=final_params[\"min_samples\"],\n",
        "    metric=\"euclidean\",\n",
        "    prediction_data=True\n",
        ")\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    top_n_words=final_params[\"top_n_words\"],\n",
        "    nr_topics=final_params[\"nr_topics\"],\n",
        "    calculate_probabilities=False,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Train model\n",
        "topics, probs = topic_model.fit_transform(docs_all, embeddings=emb_all)\n",
        "\n",
        "# Calculate NPMI\n",
        "print(\"Calculating NPMI coherence...\")\n",
        "\n",
        "try:\n",
        "    npmi_docs = docs_all[:final_params[\"npmi_docs\"]]\n",
        "    vectorizer = topic_model.vectorizer_model\n",
        "    analyzer = vectorizer.build_analyzer()\n",
        "    tokenized_docs = [analyzer(doc) for doc in npmi_docs]\n",
        "    dictionary = corpora.Dictionary(tokenized_docs)\n",
        "    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_docs]\n",
        "\n",
        "    topics_words = []\n",
        "    for topic_id in range(len(topic_model.get_topics())):\n",
        "        topic_words = topic_model.get_topic(topic_id)\n",
        "        if topic_words:\n",
        "            words = [w for w, _ in topic_words[:10]]\n",
        "            topics_words.append(words)\n",
        "\n",
        "    cm = CoherenceModel(\n",
        "        topics=topics_words,\n",
        "        texts=tokenized_docs,\n",
        "        dictionary=dictionary,\n",
        "        corpus=corpus,\n",
        "        coherence='c_npmi'\n",
        "    )\n",
        "    c_npmi_score = cm.get_coherence()\n",
        "    print(f\"NPMI coherence: {c_npmi_score:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"NPMI calculation failed: {e}\")\n",
        "    c_npmi_score = float(\"nan\")\n",
        "\n",
        "# Calculate topic diversity\n",
        "def calculate_topic_diversity(topic_model, topk=10):\n",
        "    topics = topic_model.get_topics()\n",
        "    unique_words = set()\n",
        "    total_words = 0\n",
        "    for topic_id in topics:\n",
        "        if topic_id == -1:\n",
        "            continue\n",
        "        words = [word for word, _ in topics[topic_id][:topk]]\n",
        "        unique_words.update(words)\n",
        "        total_words += len(words)\n",
        "    return len(unique_words) / total_words if total_words > 0 else 0\n",
        "\n",
        "# Save model and metrics\n",
        "print(\"Saving model and results...\")\n",
        "\n",
        "topic_model.save(f\"{OUT_DIR}/bertopic_final_model_v3\")\n",
        "topic_model.get_topic_info().to_csv(f\"{OUT_DIR}/bertopic_final_topics_v3.csv\", index=False)\n",
        "topic_model.get_document_info(docs_all).to_csv(f\"{OUT_DIR}/bertopic_final_docinfo_v3.csv\", index=False)\n",
        "\n",
        "final_metrics = {\n",
        "    \"n_topics\": len(topic_model.get_topics()) - 1,\n",
        "    \"outlier_rate\": float(np.array(topics == -1).mean()),\n",
        "    \"topic_diversity\": calculate_topic_diversity(topic_model),\n",
        "    \"c_npmi\": c_npmi_score,\n",
        "    **final_params\n",
        "}\n",
        "\n",
        "with open(f\"{OUT_DIR}/bertopic_final_metrics_v3.json\", \"w\") as f:\n",
        "    json.dump(final_metrics, f, indent=2)\n",
        "\n",
        "# Show results\n",
        "print(\"\\nFull model training completed. Results saved in:\", OUT_DIR)\n",
        "print(\"Number of topics:\", final_metrics[\"n_topics\"])\n",
        "print(\"NPMI:\", final_metrics[\"c_npmi\"])\n",
        "print(\"Outlier rate:\", final_metrics[\"outlier_rate\"])\n",
        "print(\"Diversity:\", final_metrics[\"topic_diversity\"])"
      ],
      "metadata": {
        "id": "D7Nz6KtEoiVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "import pandas as pd\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/wired_project/wired_2014_2024_cleaned.csv\"\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs = df[\"text\"].dropna().astype(str).tolist()\n",
        "\n",
        "# Download punkt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "#Word frequency statistics\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "flat_tokens = []\n",
        "for doc in docs[:30000]:\n",
        "    try:\n",
        "        words = word_tokenize(doc.lower())\n",
        "        words = [w for w in words if w.isalpha()]\n",
        "        flat_tokens.extend(words)\n",
        "    except Exception as e:\n",
        "        print(\"Text error, skipped:\", e)\n",
        "        continue\n",
        "\n",
        "counter = Counter(flat_tokens)\n",
        "common_words = counter.most_common(100)\n",
        "\n",
        "# Plot bar chart\n",
        "df_common = pd.DataFrame(common_words, columns=[\"word\", \"freq\"])\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(df_common[\"word\"], df_common[\"freq\"])\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Top 100 Frequent Words in Wired Corpus\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a53vBCBf4TNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word frequency statistics\n",
        "# Get Top 100 frequent words\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/wired_project/wired_2014_2024_cleaned.csv\"\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs = df[\"text\"].dropna().astype(str).tolist()\n",
        "\n",
        "print(f\"Loaded {len(docs):,} documents\")\n",
        "\n",
        "def get_top_words(docs, top_n=100, sample_size=10000):\n",
        "    \"\"\"Get the most frequent words\"\"\"\n",
        "    flat_tokens = []\n",
        "    sample_docs = docs[:sample_size] if len(docs) > sample_size else docs\n",
        "    print(f\"Analyzing first {len(sample_docs):,} documents...\")\n",
        "\n",
        "    for i, doc in enumerate(sample_docs):\n",
        "        if i % 2000 == 0:\n",
        "            print(f\"Progress: {i:,}/{len(sample_docs):,}\")\n",
        "        try:\n",
        "            words = re.findall(r'\\b[a-zA-Z]{2,}\\b', doc.lower())\n",
        "            flat_tokens.extend(words)\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "    counter = Counter(flat_tokens)\n",
        "    top_words = counter.most_common(top_n)\n",
        "\n",
        "    print(f\"\\nTop {top_n} frequent words:\")\n",
        "    print(\"-\" * 30)\n",
        "    for i, (word, freq) in enumerate(top_words, 1):\n",
        "        print(f\"{i:2d}. {word:<15} {freq:,}\")\n",
        "\n",
        "    return top_words\n",
        "\n",
        "top_100_words = get_top_words(docs, top_n=100)\n",
        "\n",
        "print(\"\\nSuggested stopwords list (Python set format):\")\n",
        "print(\"-\" * 50)\n",
        "words_only = [word for word, freq in top_100_words]\n",
        "print(\"custom_stopwords = {\")\n",
        "for i in range(0, len(words_only), 10):\n",
        "    chunk = words_only[i:i+10]\n",
        "    formatted_chunk = \", \".join([f\"'{word}'\" for word in chunk])\n",
        "    print(f\"    {formatted_chunk},\")\n",
        "print(\"}\")\n"
      ],
      "metadata": {
        "id": "xLK4ow06rpBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import os, json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from bertopic import BERTopic\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from umap import UMAP\n",
        "from hdbscan import HDBSCAN\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim import corpora\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/wired_project/wired_2014_2024_cleaned.csv\"\n",
        "EMBED_PATH = \"/content/drive/MyDrive/wired_project/wired_bge-base-v1.5_len512.memmap\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/wired_project\"\n",
        "DIM = 768  # embedding dimension\n",
        "\n",
        "# Custom stopwords\n",
        "custom_stopwords = {\n",
        "    'the', 'to', 'of', 'and', 'in', 'that', 'it', 'is', 'for', 'on', 'you',\n",
        "    'with', 'as', 'this', 'are', 'but', 'be', 'from', 'have', 'they', 'we',\n",
        "    'at', 'or', 'was', 'an', 'can', 'by', 'has', 'more', 'not', 'like', 'says',\n",
        "    'he', 'their', 'about', 'if', 'one', 'there', 'so', 'all', 'your', 'its',\n",
        "    'which', 'people', 'what', 'will', 'up', 'also', 'when', 'out', 'who',\n",
        "    'just', 're', 'these', 'new', 'some', 'been', 'how', 'our', 'into', 'had',\n",
        "    'than', 'would', 'his', 'other', 'them', 'time', 'us', 'were', 'could',\n",
        "    'do', 'now', 'get', 'even', 'she', 'my', 'say', 'says', 'because', 'no',\n",
        "    'most', 'over', 'wired', 'use', 'make', 'where', 'then', 'only', 'don',\n",
        "    'those', 'said', 'way', 'may', 'first', 'year', 'think', 'through', 'said',\n",
        "    'years', 'many'\n",
        "}\n",
        "\n",
        "# Merge NLTK stopwords and custom stopwords\n",
        "nltk_stopwords = set(stopwords.words('english'))\n",
        "all_stopwords = list(nltk_stopwords.union(custom_stopwords))\n",
        "\n",
        "print(f\"Total stopwords: {len(all_stopwords)}\")\n",
        "\n",
        "final_params = {\n",
        "    \"min_cluster_size\": 100,\n",
        "    \"min_samples\": 15,\n",
        "    \"n_neighbors\": 12,\n",
        "    \"min_dist\": 0.02,\n",
        "    \"n_components\": 5,\n",
        "    \"nr_topics\": \"auto\",\n",
        "    \"top_n_words\": 10,\n",
        "    \"compute_npmi\": True,\n",
        "    \"npmi_docs\": 10000\n",
        "}\n",
        "\n",
        "print(\"Loading texts...\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs_all = df[\"text\"].dropna().astype(str).tolist()\n",
        "print(f\"Number of documents: {len(docs_all):,}\")\n",
        "\n",
        "print(\"Loading embeddings...\")\n",
        "emb_all = np.memmap(EMBED_PATH, dtype=np.float32, mode=\"r\", shape=(len(docs_all), DIM))\n",
        "print(f\"Embedding matrix shape: {emb_all.shape}\")\n",
        "\n",
        "print(\"\\nTraining BERTopic model...\")\n",
        "\n",
        "umap_model = UMAP(\n",
        "    n_neighbors=final_params[\"n_neighbors\"],\n",
        "    n_components=final_params[\"n_components\"],\n",
        "    min_dist=final_params[\"min_dist\"],\n",
        "    metric=\"cosine\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "hdbscan_model = HDBSCAN(\n",
        "    min_cluster_size=final_params[\"min_cluster_size\"],\n",
        "    min_samples=final_params[\"min_samples\"],\n",
        "    metric=\"euclidean\",\n",
        "    prediction_data=True\n",
        ")\n",
        "\n",
        "vectorizer_model = CountVectorizer(\n",
        "    stop_words=all_stopwords,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=10,\n",
        "    max_df=0.9\n",
        ")\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    umap_model=umap_model,\n",
        "    hdbscan_model=hdbscan_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    top_n_words=final_params[\"top_n_words\"],\n",
        "    nr_topics=final_params[\"nr_topics\"],\n",
        "    calculate_probabilities=False,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "topics, probs = topic_model.fit_transform(docs_all, embeddings=emb_all)\n",
        "\n",
        "print(\"Calculating NPMI coherence...\")\n",
        "\n",
        "try:\n",
        "    npmi_docs = docs_all[:final_params[\"npmi_docs\"]]\n",
        "    vectorizer = topic_model.vectorizer_model\n",
        "    analyzer = vectorizer.build_analyzer()\n",
        "    tokenized_docs = [analyzer(doc) for doc in npmi_docs]\n",
        "    dictionary = corpora.Dictionary(tokenized_docs)\n",
        "    corpus = [dictionary.doc2bow(tokens) for tokens in tokenized_docs]\n",
        "\n",
        "    topics_words = []\n",
        "    for topic_id in range(len(topic_model.get_topics())):\n",
        "        topic_words = topic_model.get_topic(topic_id)\n",
        "        if topic_words:\n",
        "            words = [w for w, _ in topic_words[:10]]\n",
        "            topics_words.append(words)\n",
        "\n",
        "    cm = CoherenceModel(\n",
        "        topics=topics_words,\n",
        "        texts=tokenized_docs,\n",
        "        dictionary=dictionary,\n",
        "        corpus=corpus,\n",
        "        coherence='c_npmi'\n",
        "    )\n",
        "    c_npmi_score = cm.get_coherence()\n",
        "    print(f\"NPMI coherence: {c_npmi_score:.4f}\")\n",
        "except Exception as e:\n",
        "    print(f\"NPMI calculation failed: {e}\")\n",
        "    c_npmi_score = float(\"nan\")\n",
        "\n",
        "def calculate_topic_diversity(topic_model, topk=10):\n",
        "    topics = topic_model.get_topics()\n",
        "    unique_words = set()\n",
        "    total_words = 0\n",
        "    for topic_id in topics:\n",
        "        if topic_id == -1:\n",
        "            continue\n",
        "        words = [word for word, _ in topics[topic_id][:topk]]\n",
        "        unique_words.update(words)\n",
        "        total_words += len(words)\n",
        "    return len(unique_words) / total_words if total_words > 0 else 0\n",
        "\n",
        "print(\"Saving model and results...\")\n",
        "\n",
        "topic_model.save(f\"{OUT_DIR}/bertopic_final_model_v4_stopwords\")\n",
        "topic_model.get_topic_info().to_csv(f\"{OUT_DIR}/bertopic_final_topics_v4_stopwords.csv\", index=False)\n",
        "topic_model.get_document_info(docs_all).to_csv(f\"{OUT_DIR}/bertopic_final_docinfo_v4_stopwords.csv\", index=False)\n",
        "\n",
        "final_metrics = {\n",
        "    \"n_topics\": len(topic_model.get_topics()) - 1,\n",
        "    \"outlier_rate\": float(np.array(topics == -1).mean()),\n",
        "    \"topic_diversity\": calculate_topic_diversity(topic_model),\n",
        "    \"c_npmi\": c_npmi_score,\n",
        "    **final_params,\n",
        "    \"n_stopwords\": len(all_stopwords),\n",
        "    \"n_custom_stopwords\": len(custom_stopwords),\n",
        "    \"n_nltk_stopwords\": len(nltk_stopwords)\n",
        "}\n",
        "\n",
        "with open(f\"{OUT_DIR}/bertopic_final_metrics_v4_stopwords.json\", \"w\") as f:\n",
        "    json.dump(final_metrics, f, indent=2)\n",
        "\n",
        "print(\"\\nModel with stopwords completed. Results saved in:\", OUT_DIR)\n",
        "print(\"Number of topics:\", final_metrics[\"n_topics\"])\n",
        "print(\"NPMI:\", final_metrics[\"c_npmi\"])\n",
        "print(\"Outlier rate:\", final_metrics[\"outlier_rate\"])\n",
        "print(\"Diversity:\", final_metrics[\"topic_diversity\"])\n",
        "print(\"Total stopwords:\", final_metrics[\"n_stopwords\"])\n",
        "print(\"Custom stopwords:\", final_metrics[\"n_custom_stopwords\"])\n",
        "print(\"NLTK stopwords:\", final_metrics[\"n_nltk_stopwords\"])\n"
      ],
      "metadata": {
        "id": "QWRCV4Vv-3zi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Cv coherence\n",
        "from gensim.models import CoherenceModel\n",
        "from gensim import corpora\n",
        "\n",
        "print(\"Calculating Cv coherence...\")\n",
        "\n",
        "try:\n",
        "    cv_docs = docs_all[:30000]\n",
        "    vectorizer = topic_model.vectorizer_model\n",
        "    analyzer = vectorizer.build_analyzer()\n",
        "    tokenized_docs = [analyzer(doc) for doc in cv_docs]\n",
        "    dictionary = corpora.Dictionary(tokenized_docs)\n",
        "\n",
        "    topics_words = []\n",
        "    for topic_id in sorted(topic_model.get_topics().keys()):\n",
        "        if topic_id != -1:\n",
        "            topic_words = topic_model.get_topic(topic_id)\n",
        "            words = [w for w, _ in topic_words[:10]]\n",
        "            topics_words.append(words)\n",
        "\n",
        "    cm_cv = CoherenceModel(\n",
        "        topics=topics_words,\n",
        "        texts=tokenized_docs,\n",
        "        dictionary=dictionary,\n",
        "        coherence='c_v'\n",
        "    )\n",
        "    cv_score = cm_cv.get_coherence()\n",
        "    print(f\"Cv coherence: {cv_score:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Cv coherence calculation failed: {e}\")\n",
        "    cv_score = float(\"nan\")"
      ],
      "metadata": {
        "id": "377UUQd1IfNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# View top 10 topics\n",
        "\n",
        "from bertopic import BERTopic\n",
        "import pandas as pd\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/wired_project/bertopic_final_model_v4_stopwords\"\n",
        "\n",
        "try:\n",
        "    topic_model = BERTopic.load(model_path)\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    topic_info = topic_model.get_topic_info()\n",
        "    print(f\"Total number of topics: {len(topic_info)}\")\n",
        "    print(\"\\nTopic info table:\")\n",
        "    print(topic_info)\n",
        "\n",
        "    topic_info.to_csv('/content/drive/MyDrive/wired_project/topic_info_table.csv', index=False)\n",
        "    print(\"\\nTopic info table saved to: topic_info_table.csv\")\n",
        "\n",
        "    print(\"\\nDetails of the first 10 topics:\")\n",
        "    for i in range(min(10, len(topic_info))):\n",
        "        topic_id = topic_info.iloc[i]['Topic']\n",
        "        if topic_id != -1:\n",
        "            topic_words = topic_model.get_topic(topic_id)\n",
        "            count = topic_info.iloc[i]['Count']\n",
        "            print(f\"Topic {topic_id} (Count: {count}):\")\n",
        "            print([word for word, prob in topic_words[:10]])\n",
        "            print()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Model loading failed: {e}\")\n"
      ],
      "metadata": {
        "id": "xgZyStjAg1Hh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/wired_project\"\n",
        "\n",
        "topic_model = BERTopic.load(f\"{OUT_DIR}/bertopic_final_model_v4_stopwords\")\n",
        "print(\"Model loaded successfully\")\n",
        "\n",
        "sim_fig = topic_model.visualize_heatmap()\n",
        "sim_fig.write_html(f\"{OUT_DIR}/bertopic_similarity_heatmap_v4.html\")\n",
        "print(\"Similarity heatmap saved as HTML\")\n",
        "\n",
        "hier_fig = topic_model.visualize_hierarchy()\n",
        "hier_fig.write_html(f\"{OUT_DIR}/bertopic_hierarchy_v4.html\")\n",
        "print(\"Hierarchy visualization saved as HTML\")"
      ],
      "metadata": {
        "id": "mz779cJEEJOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 1: Sentence embedding similarity (MiniLM + seed phrases)\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Concatenate keywords into topic sentences\n",
        "topic_ids = topic_info[topic_info['Topic'] != -1]['Topic'].tolist()\n",
        "topic_texts = []\n",
        "for topic_id in topic_ids:\n",
        "    words = topic_model.get_topic(topic_id)\n",
        "    topic_texts.append(\" \".join([w[0] for w in words]))\n",
        "\n",
        "# Define seed phrases\n",
        "seed_phrases = [\n",
        "    \"concerns about risk\", \"technological threat\", \"danger of emerging technology\", \"growing public concern\",\n",
        "    \"threats from AI\", \"risks of data privacy\", \"cybersecurity failures\", \"dangers of automation\",\n",
        "    \"surveillance society\", \"algorithmic bias\", \"loss of control\", \"lack of effective regulation\",\n",
        "    \"government surveillance\", \"corporate data misuse\", \"platform manipulation\", \"unfair algorithms\",\n",
        "    \"concerns about safety\", \"digital security risks\", \"freedom under threat\", \"protection of user rights\"\n",
        "]\n",
        "\n",
        "# Load MiniLM model and encode\n",
        "sim_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "topic_embeddings = sim_model.encode(topic_texts, convert_to_tensor=True)\n",
        "seed_embeddings = sim_model.encode(seed_phrases, convert_to_tensor=True)\n",
        "\n",
        "# Compute maximum similarity\n",
        "similarities = util.cos_sim(topic_embeddings, seed_embeddings).cpu().numpy()\n",
        "max_sim_scores = np.max(similarities, axis=1)\n",
        "\n",
        "# Save results\n",
        "topic_info_embed = topic_info[topic_info[\"Topic\"] != -1].copy()\n",
        "topic_info_embed[\"embedding_similarity\"] = max_sim_scores\n",
        "\n",
        "# Filter topics\n",
        "THRESHOLD = 0.2\n",
        "filtered_embed = topic_info_embed[topic_info_embed[\"embedding_similarity\"] > THRESHOLD]\n",
        "print(f\"Embedding-based method selected {len(filtered_embed)} topics\")"
      ],
      "metadata": {
        "id": "5xjJug4fMcEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Method 2: Zero-shot classification (BART MNLI + same seed phrases)\n",
        "from transformers import pipeline\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Reuse topic_texts and seed_phrases\n",
        "zsc = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
        "\n",
        "zero_shot_scores = []\n",
        "for text in tqdm(topic_texts, desc=\"Zero-shot scoring\"):\n",
        "    result = zsc(text, seed_phrases)\n",
        "    max_score = max(result[\"scores\"])\n",
        "    zero_shot_scores.append(max_score)\n",
        "\n",
        "# Save results\n",
        "topic_info_zsc = topic_info[topic_info[\"Topic\"] != -1].copy()\n",
        "topic_info_zsc[\"zero_shot_score\"] = zero_shot_scores\n",
        "\n",
        "# Filter topics\n",
        "THRESHOLD = 0.2\n",
        "filtered_zsc = topic_info_zsc[topic_info_zsc[\"zero_shot_score\"] > THRESHOLD]\n",
        "print(f\"Zero-shot method selected {len(filtered_zsc)} topics\")\n"
      ],
      "metadata": {
        "id": "Mi5uONbkRmfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract Topic IDs\n",
        "embed_ids = set(filtered_embed[\"Topic\"])\n",
        "zsc_ids = set(filtered_zsc[\"Topic\"])\n",
        "\n",
        "# Compute intersection and union\n",
        "intersection_ids = embed_ids & zsc_ids\n",
        "union_ids = embed_ids | zsc_ids\n",
        "\n",
        "# Print results\n",
        "print(\"Intersection topic count:\", len(intersection_ids))\n",
        "print(\"Intersection Topic IDs:\", sorted(intersection_ids))\n",
        "\n",
        "print(\"\\nUnion topic count:\", len(union_ids))\n",
        "print(\"Union Topic IDs:\", sorted(union_ids))\n",
        "\n",
        "# Visualize distribution\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "venn_labels = [\"Embedding only\", \"Zero-shot only\", \"Both\"]\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.title(\"Intersection vs Union\")\n",
        "plt.bar(venn_labels, [len(embed_ids - zsc_ids), len(zsc_ids - embed_ids), len(intersection_ids)])\n",
        "plt.ylabel(\"Number of Topics\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OeyBnG_XSzW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get list of intersection Topic IDs\n",
        "intersection_ids = list(embed_ids & zsc_ids)\n",
        "\n",
        "# Print keywords for each intersected topic\n",
        "for tid in sorted(intersection_ids):\n",
        "    keywords = topic_model.get_topic(tid)\n",
        "    keywords_str = \", \".join([kw[0] for kw in keywords[:10]])\n",
        "    print(f\"Topic {tid}: {keywords_str}\")"
      ],
      "metadata": {
        "id": "Vw2Wm99GVB8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-cluster topics\n",
        "\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/wired_project\"\n",
        "topic_model = BERTopic.load(f\"{OUT_DIR}/bertopic_final_model_v4_stopwords\")\n",
        "\n",
        "target_topic_ids = [\n",
        "    0, 1, 4, 5, 7, 8, 12, 13, 14, 16, 17, 18, 21,\n",
        "    23, 24, 26, 28, 31, 32, 33, 36, 42, 44, 49, 52,\n",
        "    55, 56, 58, 59, 61, 62, 63, 64\n",
        "]\n",
        "\n",
        "# Extract keywords text for each topic\n",
        "topic_texts = []\n",
        "for tid in target_topic_ids:\n",
        "    kws = topic_model.get_topic(tid)\n",
        "    topic_texts.append(\" \".join([w for w, _ in kws[:10]]))\n",
        "\n",
        "# Sentence embeddings\n",
        "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "embeddings = embed_model.encode(topic_texts, show_progress_bar=False)\n",
        "\n",
        "# KMeans clustering (K=5)\n",
        "n_clusters = 5\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=\"auto\")\n",
        "labels = kmeans.fit_predict(embeddings)\n",
        "\n",
        "# Result table\n",
        "cluster_df = pd.DataFrame({\n",
        "    \"Topic_ID\": target_topic_ids,\n",
        "    \"Keywords\": topic_texts,\n",
        "    \"Cluster\": labels\n",
        "})\n",
        "\n",
        "# Print topics per cluster\n",
        "for i in range(n_clusters):\n",
        "    subset = cluster_df[cluster_df[\"Cluster\"] == i]\n",
        "    print(f\"\\nCluster {i} ({len(subset)} topics)\")\n",
        "    for _, r in subset.iterrows():\n",
        "        print(f\"  - Topic {r['Topic_ID']}: {r['Keywords']}\")\n",
        "\n",
        "# PCA 2D visualization\n",
        "pca = PCA(n_components=2, random_state=42)\n",
        "xy = pca.fit_transform(embeddings)\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(n_clusters):\n",
        "    pts = xy[labels == i]\n",
        "    plt.scatter(pts[:, 0], pts[:, 1], label=f\"Cluster {i}\")\n",
        "plt.title(\"Topic-level clustering (K=5)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Save Topicâ†’Cluster mapping\n",
        "topic_to_cluster = dict(zip(cluster_df[\"Topic_ID\"], cluster_df[\"Cluster\"]))\n"
      ],
      "metadata": {
        "id": "apWvMez9ccFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from umap import UMAP\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/wired_project/wired_2014_2024_cleaned.csv\"\n",
        "EMBED_PATH = \"/content/drive/MyDrive/wired_project/wired_bge-base-v1.5_len512.memmap\"\n",
        "DIM = 768\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs_all = df[\"text\"].dropna().astype(str).tolist()\n",
        "emb_all = np.memmap(EMBED_PATH, dtype=np.float32, mode=\"r\", shape=(len(docs_all), DIM))\n",
        "\n",
        "# Get topic assignment for each document\n",
        "doc_info = topic_model.get_document_info(docs_all)\n",
        "\n",
        "# Keep only target topic documents\n",
        "mask = doc_info[\"Topic\"].isin(target_topic_ids)\n",
        "doc_subset = doc_info[mask].copy()\n",
        "\n",
        "# Map topics to cluster IDs\n",
        "doc_subset[\"Cluster\"] = doc_subset[\"Topic\"].map(topic_to_cluster)\n",
        "\n",
        "# Map cluster IDs to names\n",
        "cluster_names = {\n",
        "    0: \"Mobility and Digital Infrastructure\",\n",
        "    1: \"Surveillance, Privacy and Digital Governance\",\n",
        "    2: \"Emerging Tech and Entertainment\",\n",
        "    3: \"Digital Lifestyle and Consumer Tech\",\n",
        "    4: \"Technology Policy and Communication Systems\"\n",
        "}\n",
        "doc_subset[\"Cluster_Name\"] = doc_subset[\"Cluster\"].map(cluster_names)\n",
        "\n",
        "# UMAP reduction\n",
        "umap_model = UMAP(n_neighbors=15, min_dist=0.1, n_components=2, metric=\"cosine\", random_state=42)\n",
        "doc_embeddings = emb_all[doc_subset.index]\n",
        "xy = umap_model.fit_transform(doc_embeddings)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "for cid, cname in cluster_names.items():\n",
        "    mask = doc_subset[\"Cluster\"] == cid\n",
        "    plt.scatter(\n",
        "        xy[mask, 0], xy[mask, 1],\n",
        "        s=3, alpha=0.6,\n",
        "        label=cname\n",
        "    )\n",
        "\n",
        "plt.title(\"UMAP of Wired Articles in Tech-Risk Topics\")\n",
        "plt.xlabel(\"UMAP-1\")\n",
        "plt.ylabel(\"UMAP-2\")\n",
        "plt.legend(title=\"Cluster\", fontsize=9)\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"/content/drive/MyDrive/wired_project/cluster_umap_named.png\", dpi=300)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "UhMRGzUR9uo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bertopic import BERTopic\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "\n",
        "topic_model = BERTopic.load(f\"{OUT_DIR}/bertopic_final_model_v4_stopwords\")\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs = df[\"text\"].dropna().astype(str).tolist()\n",
        "timestamps = df[\"date\"].tolist()\n",
        "\n",
        "print(\"Model and data loaded\")\n",
        "print(f\"Number of documents: {len(docs)}\")\n",
        "\n",
        "def analyze_cluster_1_basic():\n",
        "    \"\"\"Basic analysis of Cluster 1: Surveillance, Privacy and Digital Governance\"\"\"\n",
        "\n",
        "    print(\"Cluster 1 Analysis: Surveillance, Privacy and Digital Governance\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    cluster_1_topics = [7, 12, 23, 26, 28, 32, 33, 36, 52, 55, 62, 63]\n",
        "\n",
        "    cluster_docs = []\n",
        "    for i, topic_id in enumerate(topic_model.topics_):\n",
        "        if topic_id in cluster_1_topics:\n",
        "            cluster_docs.append({\n",
        "                'doc_id': i,\n",
        "                'text': docs[i],\n",
        "                'timestamp': pd.to_datetime(timestamps[i]),\n",
        "                'topic_id': topic_id,\n",
        "                'year': pd.to_datetime(timestamps[i]).year,\n",
        "                'month': pd.to_datetime(timestamps[i]).month\n",
        "            })\n",
        "\n",
        "    cluster_df = pd.DataFrame(cluster_docs)\n",
        "\n",
        "    print(f\"Total documents: {len(cluster_df)}\")\n",
        "    print(f\"Number of topics: {len(cluster_1_topics)}\")\n",
        "    print(f\"Time span: {cluster_df['year'].min()} - {cluster_df['year'].max()}\")\n",
        "\n",
        "    print(f\"\\nTOPIC BREAKDOWN:\")\n",
        "    topic_analysis = {}\n",
        "    for topic_id in cluster_1_topics:\n",
        "        topic_docs = cluster_df[cluster_df['topic_id'] == topic_id]\n",
        "        topic_keywords = topic_model.get_topic(topic_id)\n",
        "\n",
        "        topic_analysis[topic_id] = {\n",
        "            'doc_count': len(topic_docs),\n",
        "            'keywords': [word for word, score in topic_keywords[:5]],\n",
        "            'peak_year': topic_docs.groupby('year').size().idxmax() if len(topic_docs) > 0 else None,\n",
        "            'avg_doc_length': np.mean([len(doc.split()) for doc in topic_docs['text']]) if len(topic_docs) > 0 else 0\n",
        "        }\n",
        "\n",
        "        keywords_str = ', '.join(topic_analysis[topic_id]['keywords'])\n",
        "        print(f\"Topic {topic_id}: {len(topic_docs)} docs - {keywords_str}\")\n",
        "        if topic_analysis[topic_id]['peak_year']:\n",
        "            print(f\"  Peak year: {topic_analysis[topic_id]['peak_year']}\")\n",
        "\n",
        "    yearly_counts = cluster_df.groupby('year').size().reset_index(name='count')\n",
        "    peak_years = yearly_counts.nlargest(3, 'count')\n",
        "\n",
        "    print(f\"\\nTEMPORAL PATTERNS:\")\n",
        "    print(\"Peak years:\")\n",
        "    for _, row in peak_years.iterrows():\n",
        "        print(f\"  {row['year']}: {row['count']} articles\")\n",
        "\n",
        "    key_events = identify_key_events_cluster_1(yearly_counts)\n",
        "\n",
        "    create_cluster_1_visualizations(cluster_df, yearly_counts, topic_analysis)\n",
        "\n",
        "    return {\n",
        "        'cluster_df': cluster_df,\n",
        "        'topic_analysis': topic_analysis,\n",
        "        'yearly_counts': yearly_counts,\n",
        "        'key_events': key_events\n",
        "    }\n",
        "\n",
        "def identify_key_events_cluster_1(yearly_counts):\n",
        "    \"\"\"Identify key events that may explain temporal patterns\"\"\"\n",
        "\n",
        "    key_events = {\n",
        "        2013: \"Edward Snowden NSA revelations\",\n",
        "        2016: \"Cambridge Analytica data collection begins\",\n",
        "        2018: \"Cambridge Analytica scandal breaks, GDPR implemented\",\n",
        "        2019: \"Facial recognition controversies intensify\",\n",
        "        2020: \"COVID-19 surveillance concerns, contact tracing debates\"\n",
        "    }\n",
        "\n",
        "    events_analysis = []\n",
        "    for year, event in key_events.items():\n",
        "        if year in yearly_counts['year'].values:\n",
        "            count = yearly_counts[yearly_counts['year'] == year]['count'].iloc[0]\n",
        "            events_analysis.append({\n",
        "                'year': year,\n",
        "                'event': event,\n",
        "                'article_count': count\n",
        "            })\n",
        "\n",
        "    print(f\"\\nKEY EVENTS CORRELATION:\")\n",
        "    for event in events_analysis:\n",
        "        print(f\"  {event['year']}: {event['event']} - {event['article_count']} articles\")\n",
        "\n",
        "    return events_analysis\n",
        "\n",
        "def create_cluster_1_visualizations(cluster_df, yearly_counts, topic_analysis):\n",
        "    \"\"\"Create comprehensive visualizations for Cluster 1\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "    # Temporal trends with key events\n",
        "    axes[0, 0].plot(yearly_counts['year'], yearly_counts['count'],\n",
        "                    marker='o', linewidth=2, markersize=6, color='darkred')\n",
        "    key_event_years = [2013, 2018]\n",
        "    for year in key_event_years:\n",
        "        if year in yearly_counts['year'].values:\n",
        "            count = yearly_counts[yearly_counts['year'] == year]['count'].iloc[0]\n",
        "            axes[0, 0].annotate(f'{year}\\nKey Event',\n",
        "                               xy=(year, count), xytext=(year, count+5),\n",
        "                               arrowprops=dict(arrowstyle='->', color='red'),\n",
        "                               ha='center', fontsize=8)\n",
        "    axes[0, 0].set_title('Cluster 1: Temporal Trends with Key Events')\n",
        "    axes[0, 0].set_xlabel('Year')\n",
        "    axes[0, 0].set_ylabel('Number of Articles')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Topic distribution (top 8)\n",
        "    topic_counts = {tid: analysis['doc_count'] for tid, analysis in topic_analysis.items()}\n",
        "    sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    topic_ids, counts = zip(*sorted_topics[:8])\n",
        "    axes[0, 1].bar(range(len(topic_ids)), counts, color='steelblue')\n",
        "    axes[0, 1].set_title('Topic Distribution (Top 8)')\n",
        "    axes[0, 1].set_xlabel('Topic ID')\n",
        "    axes[0, 1].set_ylabel('Document Count')\n",
        "    axes[0, 1].set_xticks(range(len(topic_ids)))\n",
        "    axes[0, 1].set_xticklabels(topic_ids, rotation=45)\n",
        "\n",
        "    # Monthly trends in peak years\n",
        "    peak_years_list = [2013, 2018, 2019]\n",
        "    monthly_data = cluster_df[cluster_df['year'].isin(peak_years_list)]\n",
        "    monthly_counts = monthly_data.groupby(['year', 'month']).size().reset_index(name='count')\n",
        "    for year in peak_years_list:\n",
        "        year_data = monthly_counts[monthly_counts['year'] == year]\n",
        "        if len(year_data) > 0:\n",
        "            axes[0, 2].plot(year_data['month'], year_data['count'],\n",
        "                           marker='o', label=f'{year}', linewidth=2)\n",
        "    axes[0, 2].set_title('Monthly Trends (Peak Years)')\n",
        "    axes[0, 2].set_xlabel('Month')\n",
        "    axes[0, 2].set_ylabel('Articles')\n",
        "    axes[0, 2].legend()\n",
        "    axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Topic-year heatmap\n",
        "    topic_year = cluster_df.groupby(['year', 'topic_id']).size().unstack(fill_value=0)\n",
        "    top_5_topics = [tid for tid, _ in sorted_topics[:5]]\n",
        "    sns.heatmap(topic_year[top_5_topics].T, annot=True, fmt='d',\n",
        "                ax=axes[1, 0], cmap='Reds', cbar_kws={'label': 'Article Count'})\n",
        "    axes[1, 0].set_title('Topic Activity Heatmap (Top 5 Topics)')\n",
        "    axes[1, 0].set_xlabel('Year')\n",
        "    axes[1, 0].set_ylabel('Topic ID')\n",
        "\n",
        "    # Document length distribution\n",
        "    doc_lengths = [len(doc.split()) for doc in cluster_df['text']]\n",
        "    axes[1, 1].hist(doc_lengths, bins=25, alpha=0.7, color='lightgreen', edgecolor='black')\n",
        "    axes[1, 1].axvline(np.mean(doc_lengths), color='red', linestyle='--',\n",
        "                       label=f'Mean: {np.mean(doc_lengths):.0f} words')\n",
        "    axes[1, 1].set_title('Document Length Distribution')\n",
        "    axes[1, 1].set_xlabel('Word Count')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    # Frequent keywords\n",
        "    all_keywords = []\n",
        "    for topic_id, analysis in topic_analysis.items():\n",
        "        for keyword in analysis['keywords']:\n",
        "            all_keywords.extend([keyword] * analysis['doc_count'])\n",
        "    keyword_counts = Counter(all_keywords)\n",
        "    top_keywords = keyword_counts.most_common(10)\n",
        "    keywords, counts = zip(*top_keywords)\n",
        "    axes[1, 2].barh(range(len(keywords)), counts, color='orange')\n",
        "    axes[1, 2].set_title('Most Frequent Keywords')\n",
        "    axes[1, 2].set_xlabel('Frequency')\n",
        "    axes[1, 2].set_yticks(range(len(keywords)))\n",
        "    axes[1, 2].set_yticklabels(keywords)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "print(\"Starting analysis of Cluster 1...\")\n",
        "cluster_1_results = analyze_cluster_1_basic()"
      ],
      "metadata": {
        "id": "iNQYEu0Ca41V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from bertopic import BERTopic\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/wired_project\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/wired_project/wired_2014_2024_cleaned.csv\"\n",
        "\n",
        "topic_model = BERTopic.load(f\"{OUT_DIR}/bertopic_final_model_v4_stopwords\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs = df[\"text\"].dropna().astype(str).tolist()\n",
        "timestamps = df[\"date\"].tolist()\n",
        "doc_info = topic_model.get_document_info(docs)\n",
        "\n",
        "print(\"Model and data reloaded\")\n",
        "print(f\"Total documents: {len(docs)}\")\n",
        "print(f\"Number of topics: {len(topic_model.get_topics())}\")\n",
        "\n",
        "def create_cluster_1_clean_visualizations():\n",
        "    \"\"\"Generate clean visualizations for Cluster 1\"\"\"\n",
        "\n",
        "    cluster_1_topics = [7, 12, 23, 26, 28, 32, 33, 36, 52, 55, 62, 63]\n",
        "    cluster_1_df = doc_info[doc_info[\"Topic\"].isin(cluster_1_topics)].copy()\n",
        "    cluster_1_df[\"text\"] = df.loc[cluster_1_df.index, \"text\"]\n",
        "    cluster_1_df[\"year\"] = pd.to_datetime(df.loc[cluster_1_df.index, \"date\"]).dt.year\n",
        "\n",
        "    print(f\"Cluster 1 documents: {len(cluster_1_df)}\")\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "\n",
        "    # Yearly trend with key events\n",
        "    yearly_counts = cluster_1_df.groupby('year').size()\n",
        "    axes[0, 0].plot(yearly_counts.index, yearly_counts.values,\n",
        "                   marker='o', linewidth=3, markersize=8, color='red')\n",
        "\n",
        "    key_events = {\n",
        "        2016: 'CA Data Collection',\n",
        "        2018: 'CA Scandal & GDPR',\n",
        "        2019: 'Facial Recognition',\n",
        "        2020: 'COVID Surveillance'\n",
        "    }\n",
        "\n",
        "    for year, event in key_events.items():\n",
        "        if year in yearly_counts.index:\n",
        "            count = yearly_counts[year]\n",
        "            axes[0, 0].annotate(event, xy=(year, count),\n",
        "                               xytext=(year, count + 60),\n",
        "                               arrowprops=dict(arrowstyle='->', color='blue', lw=2),\n",
        "                               ha='center', fontsize=9,\n",
        "                               bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightyellow\"))\n",
        "\n",
        "    axes[0, 0].set_title('Cluster 1: Surveillance Risk Coverage Timeline', fontweight='bold')\n",
        "    axes[0, 0].set_xlabel('Year')\n",
        "    axes[0, 0].set_ylabel('Number of Articles')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "    max_count = yearly_counts.max()\n",
        "    axes[0, 0].set_ylim(0, max_count * 1.3)\n",
        "\n",
        "    # Topic distribution\n",
        "    topic_doc_counts = {}\n",
        "    for topic_id in cluster_1_topics:\n",
        "        topic_docs = cluster_1_df[cluster_1_df['Topic'] == topic_id]\n",
        "        topic_doc_counts[topic_id] = len(topic_docs)\n",
        "\n",
        "    sorted_topics = sorted(topic_doc_counts.items(), key=lambda x: x[1], reverse=True)[:8]\n",
        "    topic_ids, doc_counts = zip(*sorted_topics)\n",
        "\n",
        "    bars = axes[0, 1].bar(range(len(topic_ids)), doc_counts,\n",
        "                         color='steelblue', alpha=0.8)\n",
        "    axes[0, 1].set_title('Topic Distribution in Surveillance Cluster', fontweight='bold')\n",
        "    axes[0, 1].set_xlabel('Topic ID')\n",
        "    axes[0, 1].set_ylabel('Document Count')\n",
        "    axes[0, 1].set_xticks(range(len(topic_ids)))\n",
        "    axes[0, 1].set_xticklabels([f'T{tid}' for tid in topic_ids], rotation=45)\n",
        "\n",
        "    for bar, count in zip(bars, doc_counts):\n",
        "        axes[0, 1].text(bar.get_x() + bar.get_width()/2,\n",
        "                       bar.get_height() + max(doc_counts)*0.02,\n",
        "                       str(count), ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "    # Evolution of top 5 topics\n",
        "    top_5_topics = [tid for tid, _ in sorted_topics[:5]]\n",
        "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
        "\n",
        "    for i, topic_id in enumerate(top_5_topics):\n",
        "        topic_docs = cluster_1_df[cluster_1_df['Topic'] == topic_id]\n",
        "        if len(topic_docs) > 0:\n",
        "            topic_yearly = topic_docs.groupby('year').size()\n",
        "            axes[1, 0].plot(topic_yearly.index, topic_yearly.values,\n",
        "                           marker='o', label=f'Topic {topic_id}',\n",
        "                           linewidth=2, color=colors[i])\n",
        "\n",
        "    axes[1, 0].set_title('Top 5 Topics Evolution', fontweight='bold')\n",
        "    axes[1, 0].set_xlabel('Year')\n",
        "    axes[1, 0].set_ylabel('Articles per Topic')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Document length distribution\n",
        "    cluster_1_df['word_count'] = cluster_1_df['text'].apply(lambda x: len(str(x).split()))\n",
        "    axes[1, 1].hist(cluster_1_df['word_count'], bins=30,\n",
        "                   color='lightgreen', alpha=0.7, edgecolor='black')\n",
        "    mean_length = cluster_1_df['word_count'].mean()\n",
        "    axes[1, 1].axvline(mean_length, color='red', linestyle='--', linewidth=2,\n",
        "                      label=f'Mean: {mean_length:.0f} words')\n",
        "    axes[1, 1].set_title('Article Length Distribution', fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Word Count')\n",
        "    axes[1, 1].set_ylabel('Frequency')\n",
        "    axes[1, 1].legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print statistics\n",
        "    print(\"\\nSTATISTICS FOR WRITING:\")\n",
        "    print(\"=\"*40)\n",
        "    print(\"Top Topics:\")\n",
        "    for i, (topic_id, count) in enumerate(sorted_topics[:5], 1):\n",
        "        percentage = (count / len(cluster_1_df)) * 100\n",
        "        print(f\"  {i}. Topic {topic_id}: {count} docs ({percentage:.1f}%)\")\n",
        "\n",
        "    print(f\"\\nTemporal Pattern:\")\n",
        "    print(f\"  Peak year: {yearly_counts.idxmax()} ({yearly_counts.max()} articles)\")\n",
        "    print(f\"  Total span: {cluster_1_df['year'].min()}-{cluster_1_df['year'].max()}\")\n",
        "\n",
        "create_cluster_1_clean_visualizations()\n"
      ],
      "metadata": {
        "id": "v1Bq8DiHjZhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NER\n",
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_trf\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from spacy.pipeline import EntityRuler\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from itertools import combinations\n",
        "import matplotlib.pyplot as plt\n",
        "from bertopic import BERTopic\n",
        "\n",
        "topic_model = BERTopic.load(f\"{OUT_DIR}/bertopic_final_model_v4_stopwords\")\n",
        "\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs = df[\"text\"].dropna().astype(str).tolist()\n",
        "\n",
        "print(\"Data and model loaded, number of documents:\", len(docs))\n",
        "\n",
        "# Extract Cluster 1 documents\n",
        "cluster_1_topics = [7, 12, 23, 26, 28, 32, 33, 36, 52, 55, 62, 63]\n",
        "doc_info = topic_model.get_document_info(docs)\n",
        "cluster_df = doc_info[doc_info[\"Topic\"].isin(cluster_1_topics)].copy()\n",
        "cluster_df[\"text\"] = df.loc[cluster_df.index, \"text\"]\n",
        "cluster_df[\"year\"] = pd.to_datetime(df.loc[cluster_df.index, \"date\"]).dt.year\n",
        "\n",
        "print(\"Cluster 1 documents:\", len(cluster_df))\n",
        "cluster_df.head(2)\n",
        "\n",
        "# Configure spaCy pipeline (Transformer + EntityRuler)\n",
        "nlp = spacy.load(\"en_core_web_trf\", disable=[\"tagger\", \"parser\", \"lemmatizer\"])\n",
        "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
        "\n",
        "patterns = [\n",
        "    {\"label\": \"LAW\", \"pattern\": \"GDPR\"},\n",
        "    {\"label\": \"LAW\", \"pattern\": \"General Data Protection Regulation\"},\n",
        "    {\"label\": \"ORG\", \"pattern\": \"Cambridge Analytica\"},\n",
        "    {\"label\": \"ORG\", \"pattern\": \"NSA\"},\n",
        "    {\"label\": \"ORG\", \"pattern\": \"Wikileaks\"},\n",
        "    {\"label\": \"ORG\", \"pattern\": \"Clearview AI\"},\n",
        "    {\"label\": \"ORG\", \"pattern\": \"NSO Group\"},\n",
        "    {\"label\": \"PRODUCT\", \"pattern\": \"Pegasus\"},\n",
        "    {\"label\": \"ORG\", \"pattern\": \"Meta\"},\n",
        "    {\"label\": \"ORG\", \"pattern\": \"Facebook\"},\n",
        "    {\"label\": \"ORG\", \"pattern\": \"WhatsApp\"},\n",
        "    {\"label\": \"ORG\", \"pattern\": \"TikTok\"},\n",
        "    {\"label\": \"ORG\", \"pattern\": \"EU\"},\n",
        "    {\"label\": \"GPE\", \"pattern\": \"European Union\"},\n",
        "    {\"label\": \"PERSON\", \"pattern\": \"Edward Snowden\"},\n",
        "    {\"label\": \"PERSON\", \"pattern\": \"Julian Assange\"},\n",
        "    {\"label\": \"PERSON\", \"pattern\": \"Mark Zuckerberg\"},\n",
        "    {\"label\": \"TECH\", \"pattern\": \"end-to-end encryption\"},\n",
        "    {\"label\": \"TECH\", \"pattern\": \"facial recognition\"},\n",
        "    {\"label\": \"TECH\", \"pattern\": \"contact tracing\"},\n",
        "    {\"label\": \"TECH\", \"pattern\": \"net neutrality\"},\n",
        "]\n",
        "ruler.add_patterns(patterns)\n",
        "\n",
        "alias_map = {\n",
        "    \"EU\": \"European Union\",\n",
        "    \"European Union\": \"European Union\",\n",
        "    \"UK\": \"United Kingdom\",\n",
        "    \"U.K.\": \"United Kingdom\",\n",
        "    \"US\": \"United States\",\n",
        "    \"U.S.\": \"United States\",\n",
        "    \"Meta\": \"Facebook\",\n",
        "    \"Facebook\": \"Facebook\",\n",
        "    \"General Data Protection Regulation\": \"GDPR\",\n",
        "}\n",
        "\n",
        "def canonical(name: str) -> str:\n",
        "    return alias_map.get(name, name)\n",
        "\n",
        "# Run NER\n",
        "rows = []\n",
        "for doc, (_, row) in tqdm(\n",
        "    zip(nlp.pipe(cluster_df[\"text\"].tolist(), batch_size=16), cluster_df.iterrows()),\n",
        "    total=len(cluster_df)\n",
        "):\n",
        "    y = int(row[\"year\"])\n",
        "    ents = [(canonical(ent.text), ent.label_) for ent in doc.ents]\n",
        "    ents = [(t, l) for (t, l) in ents if l in {\"PERSON\", \"ORG\", \"GPE\", \"LAW\", \"PRODUCT\", \"NORP\", \"TECH\"}]\n",
        "    if ents:\n",
        "        rows.append({\"year\": y, \"ents\": ents})\n",
        "\n",
        "ner_df = pd.DataFrame(rows)\n",
        "print(\"NER completed, processed documents:\", len(ner_df))\n",
        "\n",
        "# Frequent entity statistics\n",
        "counter_all = Counter()\n",
        "for ents in ner_df[\"ents\"]:\n",
        "    counter_all.update([e[0] for e in ents])\n",
        "\n",
        "top_entities = counter_all.most_common(20)\n",
        "print(\"Top 20 entities:\")\n",
        "print(pd.DataFrame(top_entities, columns=[\"entity\", \"count\"]))\n",
        "\n",
        "# Temporal trends\n",
        "def entity_year_counts(targets=None, top_k=15):\n",
        "    df_list = []\n",
        "    for year, ents in zip(ner_df[\"year\"], ner_df[\"ents\"]):\n",
        "        for t, l in ents:\n",
        "            df_list.append((year, t, l))\n",
        "    edf = pd.DataFrame(df_list, columns=[\"year\", \"entity\", \"label\"])\n",
        "    if targets is None:\n",
        "        targets = [e for e, _ in counter_all.most_common(top_k)]\n",
        "    sub = edf[edf[\"entity\"].isin(targets)]\n",
        "    pivot = sub.groupby([\"year\", \"entity\"]).size().unstack(fill_value=0).sort_index()\n",
        "    return pivot\n",
        "\n",
        "trend = entity_year_counts(targets=[\"Facebook\", \"Cambridge Analytica\", \"GDPR\", \"Edward Snowden\", \"NSA\", \"facial recognition\"])\n",
        "trend.plot(figsize=(10, 6), marker=\"o\", title=\"Cluster 1 - Entity Temporal Trends\")\n",
        "plt.show()\n",
        "\n",
        "# Co-occurrence network\n",
        "co_counter = Counter()\n",
        "for ents in ner_df[\"ents\"]:\n",
        "    uniq = sorted(set([e[0] for e in ents]))\n",
        "    for a, b in combinations(uniq, 2):\n",
        "        co_counter[(a, b)] += 1\n",
        "\n",
        "edges = pd.DataFrame([(a, b, w) for (a, b), w in co_counter.items()],\n",
        "                     columns=[\"source\", \"target\", \"weight\"])\n",
        "print(\"Top 20 co-occurrence relations:\")\n",
        "print(edges.sort_values(\"weight\", ascending=False).head(20))\n"
      ],
      "metadata": {
        "id": "eiLXmMljjZrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster 1 spaCy analysis\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "from collections import Counter, defaultdict\n",
        "from bertopic import BERTopic\n",
        "\n",
        "OUT_DIR = \"/content/drive/MyDrive/wired_project\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/wired_project/wired_2014_2024_cleaned.csv\"\n",
        "\n",
        "topic_model = BERTopic.load(f\"{OUT_DIR}/bertopic_final_model_v4_stopwords\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs = df[\"text\"].dropna().astype(str).tolist()\n",
        "\n",
        "print(\"Model and data reloaded, number of documents:\", len(docs))\n",
        "\n",
        "cluster_1_topics = [7, 12, 23, 26, 28, 32, 33, 36, 52, 55, 62, 63]\n",
        "doc_info = topic_model.get_document_info(docs)\n",
        "cluster_df = doc_info[doc_info[\"Topic\"].isin(cluster_1_topics)].copy()\n",
        "cluster_df[\"text\"] = df.loc[cluster_df.index, \"text\"]\n",
        "cluster_df[\"year\"] = pd.to_datetime(df.loc[cluster_df.index, \"date\"]).dt.year\n",
        "\n",
        "print(\"Cluster 1 documents:\", len(cluster_df))\n",
        "\n",
        "def improved_spacy_analysis(cluster_df, sample_size=1000):\n",
        "    \"\"\"Improved spaCy analysis\"\"\"\n",
        "\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    def filter_entity(ent):\n",
        "        text = ent.text.strip().lower()\n",
        "        if len(text) < 2:\n",
        "            return False\n",
        "        if text in ['us', 'u.s.']:\n",
        "            return False\n",
        "        if 'wired' in text:\n",
        "            return False\n",
        "        if text.isdigit():\n",
        "            return False\n",
        "        if text in ['new', 'first', 'last', 'good', 'best', 'old', 'big', 'small']:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def filter_cooccurrence(ent1, ent2):\n",
        "        if 'wired' in ent1.lower() or 'wired' in ent2.lower():\n",
        "            return False\n",
        "        if ent1.lower() == ent2.lower():\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    print(f\"Starting improved spaCy analysis, sample size: {min(sample_size, len(cluster_df))}\")\n",
        "\n",
        "    if sample_size < len(cluster_df):\n",
        "        sample_docs = cluster_df.sample(n=sample_size, random_state=42)\n",
        "    else:\n",
        "        sample_docs = cluster_df\n",
        "\n",
        "    entity_counts = Counter()\n",
        "    cooccurrence_counts = defaultdict(int)\n",
        "    processed = 0\n",
        "\n",
        "    for idx, row in tqdm(sample_docs.iterrows(), total=len(sample_docs)):\n",
        "        try:\n",
        "            doc = nlp(row['text'][:3000])\n",
        "\n",
        "            filtered_entities = []\n",
        "            for ent in doc.ents:\n",
        "                if ent.label_ in ['PERSON', 'ORG', 'GPE'] and filter_entity(ent):\n",
        "                    entity_text = ent.text.strip()\n",
        "                    entity_counts[entity_text] += 1\n",
        "                    filtered_entities.append(entity_text)\n",
        "\n",
        "            if 1 < len(filtered_entities) < 20:\n",
        "                for i, ent1 in enumerate(filtered_entities):\n",
        "                    for ent2 in filtered_entities[i+1:]:\n",
        "                        if filter_cooccurrence(ent1, ent2):\n",
        "                            pair = tuple(sorted([ent1, ent2]))\n",
        "                            cooccurrence_counts[pair] += 1\n",
        "\n",
        "            processed += 1\n",
        "\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "    print(f\"Processing completed, documents processed: {processed}\")\n",
        "    return entity_counts, cooccurrence_counts\n",
        "\n",
        "entity_counts, cooccurrence_counts = improved_spacy_analysis(cluster_df, sample_size=1000)\n",
        "\n",
        "print(\"\\n=== Improved Entity Recognition Results ===\")\n",
        "print(\"Top 20 Entities:\")\n",
        "for i, (entity, count) in enumerate(entity_counts.most_common(20)):\n",
        "    print(f\"{i+1:2d}. {entity:<20} {count:>4}\")\n",
        "\n",
        "print(\"\\n=== Improved Co-occurrence Results ===\")\n",
        "print(\"Top 15 Entity Co-occurrences:\")\n",
        "for i, ((ent1, ent2), count) in enumerate(sorted(cooccurrence_counts.items(), key=lambda x: x[1], reverse=True)[:15]):\n",
        "    print(f\"{i+1:2d}. {ent1:<15} <-> {ent2:<15} {count:>3}\")\n"
      ],
      "metadata": {
        "id": "gYhvuJy3NWXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def power_dynamics_timeline_analysis(cluster_df):\n",
        "    \"\"\"Analyze the changing prominence of power actors over time\"\"\"\n",
        "\n",
        "    power_categories = {\n",
        "        'Tech Giants': ['google', 'apple', 'amazon', 'microsoft', 'facebook'],\n",
        "        'Government': ['fbi', 'nsa', 'congress', 'white house'],\n",
        "        'Foreign Powers': ['russia', 'china', 'uk'],\n",
        "        'Individual Actors': ['donald trump', 'clinton', 'snowden']\n",
        "    }\n",
        "\n",
        "    yearly_power_data = []\n",
        "    for year in sorted(cluster_df['year'].unique()):\n",
        "        year_docs = cluster_df[cluster_df['year'] == year]\n",
        "        year_text = ' '.join(year_docs['text'].astype(str)).lower()\n",
        "\n",
        "        year_data = {'year': year}\n",
        "        for category, entities in power_categories.items():\n",
        "            category_mentions = sum(year_text.count(entity.lower()) for entity in entities)\n",
        "            year_data[category] = category_mentions\n",
        "\n",
        "        yearly_power_data.append(year_data)\n",
        "\n",
        "    df_power = pd.DataFrame(yearly_power_data)\n",
        "\n",
        "    # Stacked area chart\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    plt.stackplot(\n",
        "        df_power['year'],\n",
        "        df_power['Tech Giants'],\n",
        "        df_power['Government'],\n",
        "        df_power['Foreign Powers'],\n",
        "        df_power['Individual Actors'],\n",
        "        labels=['Tech Giants', 'Government', 'Foreign Powers', 'Individual Actors'],\n",
        "        alpha=0.8\n",
        "    )\n",
        "    plt.title('Power Actor Prominence in Surveillance Discourse (2014-2024)')\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Mention Frequency')\n",
        "    plt.legend(loc='upper left')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Yearly power actor data:\")\n",
        "    print(df_power)\n",
        "\n",
        "    return df_power\n",
        "\n",
        "# Run analysis\n",
        "df_power = power_dynamics_timeline_analysis(cluster_df)\n"
      ],
      "metadata": {
        "id": "E_h_aK9CRC08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def emotional_authority_mapping(cluster_df, entity_counts):\n",
        "    \"\"\"Analyze emotional-authoritative discourse strategies of key entities\"\"\"\n",
        "\n",
        "    # Fear lexicon (expanded)\n",
        "    fear_words = [\n",
        "        \"fear\", \"scared\", \"scary\", \"terrified\", \"terrifying\", \"terrible\",\n",
        "        \"horror\", \"horrific\", \"horrifying\", \"fright\", \"frightening\",\n",
        "        \"panic\", \"panic-stricken\", \"panic attack\", \"phobia\",\n",
        "        \"danger\", \"dangerous\", \"risk\", \"risky\", \"hazard\", \"hazardous\",\n",
        "        \"threat\", \"threatening\", \"menace\", \"peril\", \"exposed\", \"vulnerable\",\n",
        "        \"crisis\", \"catastrophe\", \"catastrophic\", \"disaster\", \"calamity\",\n",
        "        \"apocalypse\", \"collapse\", \"devastating\", \"devastated\", \"tragedy\",\n",
        "        \"alarm\", \"alarming\", \"fearful\", \"fearsome\", \"dread\", \"dreadful\",\n",
        "        \"panic\", \"shock\", \"shocking\", \"nightmare\", \"chaos\", \"uncertainty\",\n",
        "        \"doom\", \"insecure\", \"frightened\", \"scare\", \"menacing\", \"scaremongering\"\n",
        "    ]\n",
        "\n",
        "    # Authority / evidence lexicon\n",
        "    authority_words = [\n",
        "        \"official\", \"officials\", \"government\", \"administration\",\n",
        "        \"authorities\", \"agency\", \"organization\", \"institution\",\n",
        "        \"committee\", \"commission\", \"department\", \"ministry\",\n",
        "        \"expert\", \"experts\", \"specialist\", \"specialists\",\n",
        "        \"scientist\", \"scientists\", \"researcher\", \"researchers\",\n",
        "        \"academics\", \"professor\", \"scholars\", \"analyst\", \"analysts\",\n",
        "        \"research\", \"study\", \"studies\", \"study shows\", \"studies show\",\n",
        "        \"data\", \"data reveals\", \"statistics\", \"findings\", \"report\", \"reports\",\n",
        "        \"survey\", \"evidence\", \"confirmed\", \"verified\", \"according\", \"according to\",\n",
        "        \"official data\", \"statement\", \"authoritative\", \"documentation\"\n",
        "    ]\n",
        "\n",
        "    top_entities = [entity for entity, count in entity_counts.most_common(12)]\n",
        "    entity_profiles = []\n",
        "\n",
        "    print(\"Processing entities for emotional-authority mapping...\")\n",
        "\n",
        "    for entity in top_entities:\n",
        "        entity_docs = cluster_df[\n",
        "            cluster_df['text'].str.contains(entity, case=False, na=False)\n",
        "        ]\n",
        "        if len(entity_docs) == 0:\n",
        "            continue\n",
        "\n",
        "        combined_text = ' '.join(entity_docs['text'].head(50)).lower()\n",
        "        fear_score = sum(combined_text.count(word) for word in fear_words)\n",
        "        authority_score = sum(combined_text.count(word) for word in authority_words)\n",
        "\n",
        "        doc_count = len(entity_docs)\n",
        "        word_count = len(combined_text.split())\n",
        "\n",
        "        fear_score_norm = (fear_score / word_count) * 1000 if word_count > 0 else 0\n",
        "        authority_score_norm = (authority_score / word_count) * 1000 if word_count > 0 else 0\n",
        "\n",
        "        entity_profiles.append({\n",
        "            'entity': entity,\n",
        "            'fear_score': fear_score_norm,\n",
        "            'authority_score': authority_score_norm,\n",
        "            'doc_count': doc_count,\n",
        "            'word_count': word_count\n",
        "        })\n",
        "\n",
        "        print(f\"  {entity}: {doc_count} docs, fear={fear_score_norm:.2f}, authority={authority_score_norm:.2f}\")\n",
        "\n",
        "    df_profiles = pd.DataFrame(entity_profiles)\n",
        "\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    def get_entity_color(entity):\n",
        "        entity_lower = entity.lower()\n",
        "        if any(tech in entity_lower for tech in ['google', 'apple', 'amazon', 'microsoft', 'facebook']):\n",
        "            return 'blue'\n",
        "        elif any(gov in entity_lower for gov in ['fbi', 'nsa', 'congress', 'white house']):\n",
        "            return 'red'\n",
        "        elif any(foreign in entity_lower for foreign in ['russia', 'china', 'uk']):\n",
        "            return 'green'\n",
        "        else:\n",
        "            return 'orange'\n",
        "\n",
        "    colors = [get_entity_color(entity) for entity in df_profiles['entity']]\n",
        "    sizes = [min(doc_count * 5, 500) for doc_count in df_profiles['doc_count']]\n",
        "\n",
        "    plt.scatter(\n",
        "        df_profiles['authority_score'],\n",
        "        df_profiles['fear_score'],\n",
        "        s=sizes,\n",
        "        c=colors,\n",
        "        alpha=0.7,\n",
        "        edgecolors='black',\n",
        "        linewidth=0.5\n",
        "    )\n",
        "\n",
        "    for _, row in df_profiles.iterrows():\n",
        "        plt.annotate(row['entity'],\n",
        "                     (row['authority_score'], row['fear_score']),\n",
        "                     xytext=(5, 5),\n",
        "                     textcoords='offset points',\n",
        "                     fontsize=9,\n",
        "                     ha='left')\n",
        "\n",
        "    plt.xlabel('Authority Language (per 1000 words)')\n",
        "    plt.ylabel('Fear Language (per 1000 words)')\n",
        "    plt.title('Discourse Strategy Mapping: Authority vs Fear in Surveillance Narratives')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "    legend_elements = [\n",
        "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=10, label='Tech Companies'),\n",
        "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=10, label='Government'),\n",
        "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='green', markersize=10, label='Foreign Powers'),\n",
        "        plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='orange', markersize=10, label='Others')\n",
        "    ]\n",
        "    plt.legend(handles=legend_elements, loc='upper right')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(\"\\nEntity Discourse Profile:\")\n",
        "    print(df_profiles.sort_values('fear_score', ascending=False))\n",
        "\n",
        "    return df_profiles\n",
        "\n",
        "# Run emotional-authority analysis\n",
        "df_profiles = emotional_authority_mapping(cluster_df, entity_counts)"
      ],
      "metadata": {
        "id": "sXeGwjjMRwUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def facebook_cambridge_analytica_deep_dive(cluster_df):\n",
        "    \"\"\"Deep dive analysis of the Facebook / Cambridge Analytica case\"\"\"\n",
        "\n",
        "    # Keywords for filtering\n",
        "    facebook_keywords = ['facebook', 'cambridge analytica', 'zuckerberg', 'fb', 'social media platform']\n",
        "    facebook_docs = cluster_df[\n",
        "        cluster_df['text'].str.contains('|'.join(facebook_keywords), case=False, na=False)\n",
        "    ].copy()\n",
        "\n",
        "    print(f\"Total Facebook-related documents: {len(facebook_docs)}\")\n",
        "\n",
        "    # Yearly counts\n",
        "    yearly_fb = facebook_docs.groupby('year').size().reset_index(name='count')\n",
        "\n",
        "    # Time periods\n",
        "    periods = {\n",
        "        'Pre-scandal (2014-2017)': facebook_docs[facebook_docs['year'] <= 2017],\n",
        "        'Scandal Peak (2018)': facebook_docs[facebook_docs['year'] == 2018],\n",
        "        'Post-scandal (2019-2024)': facebook_docs[facebook_docs['year'] >= 2019]\n",
        "    }\n",
        "\n",
        "    # Risk, sentiment, and regulatory vocabularies\n",
        "    tech_risk_words = [\n",
        "        \"privacy\", \"data breach\", \"data leak\", \"data theft\", \"hacking\", \"hackers\",\n",
        "        \"cyberattack\", \"cybersecurity\", \"phishing\", \"ransomware\", \"malware\",\n",
        "        \"virus\", \"spyware\", \"trojan\", \"breach\", \"exposed\", \"unauthorized access\",\n",
        "        \"surveillance\", \"mass surveillance\", \"facial recognition\", \"biometric data\",\n",
        "        \"tracking\", \"monitoring\", \"spying\", \"espionage\", \"censorship\", \"manipulation\",\n",
        "        \"propaganda\", \"misinformation\", \"disinformation\", \"deepfake\", \"fake news\",\n",
        "        \"algorithm\", \"bias\", \"algorithmic bias\", \"automation\", \"black box\",\n",
        "        \"opacity\", \"fairness\", \"discrimination\", \"unethical AI\", \"job loss\",\n",
        "        \"replacement\", \"autonomous\", \"lethal autonomous weapon\", \"AI ethics\",\n",
        "        \"monopoly\", \"platform power\", \"gatekeeping\", \"data monopoly\",\n",
        "        \"addiction\", \"overuse\", \"misuse\", \"platform abuse\", \"shadowban\",\n",
        "        \"exploit\", \"exploitation\", \"unsafe\", \"dangerous technology\"\n",
        "    ]\n",
        "\n",
        "    negative_words = [\n",
        "        \"scandal\", \"violation\", \"abuse\", \"concern\", \"threat\", \"dangerous\",\n",
        "        \"crisis\", \"catastrophe\", \"collapse\", \"chaos\", \"corruption\",\n",
        "        \"failure\", \"fraud\", \"harm\", \"harmful\", \"toxic\", \"controversy\",\n",
        "        \"backlash\", \"boycott\", \"risk\", \"fear\", \"alarm\", \"panic\",\n",
        "        \"deadly\", \"fatal\", \"devastating\", \"nightmare\", \"unprecedented\"\n",
        "    ]\n",
        "\n",
        "    regulatory_words = [\n",
        "        \"regulation\", \"oversight\", \"accountability\", \"transparency\", \"congress\",\n",
        "        \"parliament\", \"legislation\", \"lawmakers\", \"law\", \"bill\", \"act\",\n",
        "        \"policy\", \"governance\", \"authority\", \"commission\", \"agency\",\n",
        "        \"standards\", \"compliance\", \"ethics\", \"responsibility\",\n",
        "        \"investigation\", \"report\", \"committee\", \"task force\", \"ministry\",\n",
        "        \"federal\", \"supreme court\", \"executive order\"\n",
        "    ]\n",
        "\n",
        "    # Create figures\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Time trend\n",
        "    axes[0, 0].plot(yearly_fb['year'], yearly_fb['count'], marker='o', linewidth=2, color='blue')\n",
        "    axes[0, 0].axvline(x=2018, color='red', linestyle='--', alpha=0.7, label='CA Scandal')\n",
        "    axes[0, 0].set_title('Facebook Coverage Over Time')\n",
        "    axes[0, 0].set_xlabel('Year')\n",
        "    axes[0, 0].set_ylabel('Number of Articles')\n",
        "    axes[0, 0].legend()\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Period-based keyword analysis\n",
        "    period_keywords = {}\n",
        "    risk_evolution = {}\n",
        "\n",
        "    for period_name, period_docs in periods.items():\n",
        "        if len(period_docs) == 0:\n",
        "            continue\n",
        "\n",
        "        combined_text = ' '.join(period_docs['text']).lower()\n",
        "\n",
        "        risk_evolution[period_name] = sum(combined_text.count(word) for word in tech_risk_words)\n",
        "\n",
        "        period_keywords[period_name] = {\n",
        "            'negative_sentiment': sum(combined_text.count(word) for word in negative_words),\n",
        "            'regulatory_response': sum(combined_text.count(word) for word in regulatory_words),\n",
        "            'doc_count': len(period_docs)\n",
        "        }\n",
        "\n",
        "    # Tech risk vocabulary evolution\n",
        "    risk_df = pd.DataFrame(risk_evolution, index=['mentions']).T\n",
        "    risk_df.plot(kind='bar', ax=axes[0, 1], legend=False, color='orange')\n",
        "    axes[0, 1].set_title('Tech Risk Language Evolution')\n",
        "    axes[0, 1].set_xlabel('Time Period')\n",
        "    axes[0, 1].set_ylabel('Mention Count')\n",
        "    axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Sentiment & regulatory discourse\n",
        "    sentiment_data = pd.DataFrame(period_keywords).T\n",
        "    sentiment_data[['negative_sentiment', 'regulatory_response']].plot(kind='bar', ax=axes[1, 0])\n",
        "    axes[1, 0].set_title('Discourse Tone Evolution')\n",
        "    axes[1, 0].set_xlabel('Time Period')\n",
        "    axes[1, 0].set_ylabel('Word Count')\n",
        "    axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    # Regulatory intensity\n",
        "    reg_intensity = [data['regulatory_response']/data['doc_count'] if data['doc_count'] > 0 else 0\n",
        "                     for data in period_keywords.values()]\n",
        "    period_names = list(period_keywords.keys())\n",
        "    axes[1, 1].bar(period_names, reg_intensity, color=['lightblue', 'red', 'lightgreen'])\n",
        "    axes[1, 1].set_title('Regulatory Language Intensity')\n",
        "    axes[1, 1].set_xlabel('Time Period')\n",
        "    axes[1, 1].set_ylabel('Regulatory Words per Document')\n",
        "    axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Summary\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"FACEBOOK / CAMBRIDGE ANALYTICA CASE STUDY SUMMARY\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    for period_name, data in period_keywords.items():\n",
        "        neg_intensity = data['negative_sentiment']/data['doc_count'] if data['doc_count'] > 0 else 0\n",
        "        reg_intensity = data['regulatory_response']/data['doc_count'] if data['doc_count'] > 0 else 0\n",
        "        print(f\"\\n{period_name}:\")\n",
        "        print(f\"  Documents: {data['doc_count']}\")\n",
        "        print(f\"  Negative sentiment intensity: {neg_intensity:.2f}\")\n",
        "        print(f\"  Regulatory response intensity: {reg_intensity:.2f}\")\n",
        "\n",
        "    return facebook_docs, period_keywords\n",
        "\n",
        "# Run analysis\n",
        "facebook_docs, fb_analysis = facebook_cambridge_analytica_deep_dive(cluster_df)\n"
      ],
      "metadata": {
        "id": "ijXpiThkdl58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Reload data and model\n",
        "OUT_DIR = \"/content/drive/MyDrive/wired_project\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/wired_project/wired_2014_2024_cleaned.csv\"\n",
        "\n",
        "topic_model = BERTopic.load(f\"{OUT_DIR}/bertopic_final_model_v4_stopwords\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs = df[\"text\"].dropna().astype(str).tolist()\n",
        "\n",
        "print(\"Model and data reloaded, number of documents:\", len(docs))\n",
        "\n",
        "# Cluster 1 data\n",
        "cluster_1_topics = [7, 12, 23, 26, 28, 32, 33, 36, 52, 55, 62, 63]\n",
        "doc_info = topic_model.get_document_info(docs)\n",
        "cluster_df = doc_info[doc_info[\"Topic\"].isin(cluster_1_topics)].copy()\n",
        "cluster_df[\"text\"] = df.loc[cluster_df.index, \"text\"]\n",
        "cluster_df[\"year\"] = pd.to_datetime(df.loc[cluster_df.index, \"date\"]).dt.year\n",
        "\n",
        "print(\"Cluster 1 data prepared, documents:\", len(cluster_df))\n",
        "\n",
        "def analyze_cluster_2_complete():\n",
        "    \"\"\"Complete analysis of Cluster 2: Emerging Tech and Entertainment\"\"\"\n",
        "\n",
        "    print(\"Cluster 2 Analysis: Emerging Tech and Entertainment\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    cluster_2_topics = [0, 1, 8, 17, 24, 42, 44, 49]\n",
        "    cluster_2_df = doc_info[doc_info[\"Topic\"].isin(cluster_2_topics)].copy()\n",
        "    cluster_2_df[\"text\"] = df.loc[cluster_2_df.index, \"text\"]\n",
        "    cluster_2_df[\"year\"] = pd.to_datetime(df.loc[cluster_2_df.index, \"date\"]).dt.year\n",
        "\n",
        "    print(f\"Total documents: {len(cluster_2_df)}\")\n",
        "    print(f\"Number of topics: {len(cluster_2_topics)}\")\n",
        "    print(f\"Time span: {cluster_2_df['year'].min()} - {cluster_2_df['year'].max()}\")\n",
        "\n",
        "    print(f\"\\nTOPIC BREAKDOWN:\")\n",
        "    topic_analysis = {}\n",
        "\n",
        "    topic_descriptions = {\n",
        "        0: \"COVID-19/Medical AI/Vaccines\",\n",
        "        1: \"VR Gaming/Nintendo/Xbox\",\n",
        "        8: \"Drones/Aviation Technology\",\n",
        "        17: \"Star Wars/Entertainment Media\",\n",
        "        24: \"Camera/Photography Technology\",\n",
        "        42: \"AI Games/AlphaGo/DeepMind\",\n",
        "        44: \"Microsoft/Windows/Operating Systems\",\n",
        "        49: \"Pokemon Go/Mobile Gaming\"\n",
        "    }\n",
        "\n",
        "    for topic_id in cluster_2_topics:\n",
        "        topic_docs = cluster_2_df[cluster_2_df['Topic'] == topic_id]\n",
        "        topic_keywords = topic_model.get_topic(topic_id)\n",
        "\n",
        "        topic_analysis[topic_id] = {\n",
        "            'doc_count': len(topic_docs),\n",
        "            'keywords': [word for word, score in topic_keywords[:5]],\n",
        "            'description': topic_descriptions.get(topic_id, \"Unknown\"),\n",
        "            'peak_year': topic_docs.groupby('year').size().idxmax() if len(topic_docs) > 0 else None\n",
        "        }\n",
        "\n",
        "        keywords_str = ', '.join(topic_analysis[topic_id]['keywords'])\n",
        "        print(f\"Topic {topic_id}: {len(topic_docs)} docs - {topic_analysis[topic_id]['description']}\")\n",
        "        print(f\"  Keywords: {keywords_str}\")\n",
        "        if topic_analysis[topic_id]['peak_year']:\n",
        "            print(f\"  Peak year: {topic_analysis[topic_id]['peak_year']}\")\n",
        "\n",
        "    yearly_counts = cluster_2_df.groupby('year').size().reset_index(name='count')\n",
        "    peak_years = yearly_counts.nlargest(3, 'count')\n",
        "\n",
        "    print(f\"\\nTEMPORAL PATTERNS:\")\n",
        "    print(\"Peak years:\")\n",
        "    for _, row in peak_years.iterrows():\n",
        "        print(f\"  {row['year']}: {row['count']} articles\")\n",
        "\n",
        "    print(f\"\\nCOMPARISON WITH CLUSTER 1:\")\n",
        "    cluster_1_total = len(cluster_df)\n",
        "    cluster_2_total = len(cluster_2_df)\n",
        "    print(f\"Cluster 1 (Surveillance): {cluster_1_total} documents\")\n",
        "    print(f\"Cluster 2 (Emerging Tech): {cluster_2_total} documents\")\n",
        "    print(f\"Coverage ratio (C2/C1): {cluster_2_total/cluster_1_total:.2f}\")\n",
        "\n",
        "    create_cluster_2_visualizations(cluster_2_df, yearly_counts, topic_analysis)\n",
        "\n",
        "    return cluster_2_df, topic_analysis\n",
        "\n",
        "def create_cluster_2_visualizations(cluster_2_df, yearly_counts, topic_analysis):\n",
        "    \"\"\"Visualizations for Cluster 2\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # 1. Yearly trend\n",
        "    axes[0, 0].plot(yearly_counts['year'], yearly_counts['count'],\n",
        "                    marker='o', linewidth=2, markersize=6, color='green')\n",
        "\n",
        "    tech_events = {2016: 'AlphaGo vs Lee', 2020: 'COVID-19'}\n",
        "    for year, event in tech_events.items():\n",
        "        if year in yearly_counts['year'].values:\n",
        "            count = yearly_counts[yearly_counts['year'] == year]['count'].iloc[0]\n",
        "            axes[0, 0].annotate(f'{event}',\n",
        "                               xy=(year, count), xytext=(year, count+15),\n",
        "                               arrowprops=dict(arrowstyle='->', color='red'),\n",
        "                               ha='center', fontsize=8)\n",
        "\n",
        "    axes[0, 0].set_title('Cluster 2: Emerging Tech Coverage Over Time')\n",
        "    axes[0, 0].set_xlabel('Year')\n",
        "    axes[0, 0].set_ylabel('Number of Articles')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Topic distribution\n",
        "    topic_counts = {tid: analysis['doc_count'] for tid, analysis in topic_analysis.items()}\n",
        "    sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    topic_ids, counts = zip(*sorted_topics)\n",
        "\n",
        "    axes[0, 1].bar(range(len(topic_ids)), counts, color='lightgreen')\n",
        "    axes[0, 1].set_title('Topic Distribution in Cluster 2')\n",
        "    axes[0, 1].set_xlabel('Topic ID')\n",
        "    axes[0, 1].set_ylabel('Document Count')\n",
        "    axes[0, 1].set_xticks(range(len(topic_ids)))\n",
        "    axes[0, 1].set_xticklabels([f'T{tid}' for tid in topic_ids], rotation=45)\n",
        "\n",
        "    # 3. Tech category pie chart\n",
        "    tech_categories = {\n",
        "        'AI/ML': [0, 42],\n",
        "        'Gaming/VR': [1, 49],\n",
        "        'Hardware/Systems': [8, 24, 44],\n",
        "        'Entertainment': [17]\n",
        "    }\n",
        "\n",
        "    category_counts = {}\n",
        "    for category, topic_list in tech_categories.items():\n",
        "        count = sum(topic_analysis[tid]['doc_count'] for tid in topic_list if tid in topic_analysis)\n",
        "        category_counts[category] = count\n",
        "\n",
        "    categories, counts = zip(*category_counts.items())\n",
        "    colors = ['lightblue', 'lightcoral', 'lightgreen', 'gold']\n",
        "    axes[1, 0].pie(counts, labels=categories, autopct='%1.1f%%',\n",
        "                   startangle=90, colors=colors)\n",
        "    axes[1, 0].set_title('Technology Category Distribution')\n",
        "\n",
        "    # 4. Top 3 topics evolution\n",
        "    top_3_topics = [tid for tid, _ in sorted_topics[:3]]\n",
        "    for topic_id in top_3_topics:\n",
        "        topic_docs = cluster_2_df[cluster_2_df['Topic'] == topic_id]\n",
        "        if len(topic_docs) > 0:\n",
        "            topic_yearly = topic_docs.groupby('year').size()\n",
        "            axes[1, 1].plot(topic_yearly.index, topic_yearly.values,\n",
        "                           marker='o', label=f'Topic {topic_id}', linewidth=2)\n",
        "\n",
        "    axes[1, 1].set_title('Top 3 Topics Evolution')\n",
        "    axes[1, 1].set_xlabel('Year')\n",
        "    axes[1, 1].set_ylabel('Articles')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def compare_cluster_1_vs_2_discourse(cluster_1_df, cluster_2_df):\n",
        "    \"\"\"Compare discourse features of Cluster 1 vs Cluster 2\"\"\"\n",
        "\n",
        "    print(\"\\nCOMPARATIVE DISCOURSE ANALYSIS: Surveillance vs Emerging Tech\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    surveillance_words = [\n",
        "        \"threat\", \"risk\", \"concern\", \"danger\", \"security\", \"privacy\",\n",
        "        \"surveillance\", \"breach\", \"data breach\", \"data leak\", \"hack\",\n",
        "        \"hacking\", \"cyberattack\", \"cybersecurity\", \"ransomware\", \"malware\",\n",
        "        \"virus\", \"spyware\", \"trojan\", \"exposed\", \"unauthorized\",\n",
        "        \"monitoring\", \"tracking\", \"spying\", \"espionage\", \"censorship\",\n",
        "        \"manipulation\", \"propaganda\", \"misinformation\", \"disinformation\",\n",
        "        \"deepfake\", \"fake news\", \"bias\", \"algorithmic bias\", \"black box\",\n",
        "        \"opacity\", \"unethical\", \"unsafe\", \"harmful\", \"catastrophe\",\n",
        "        \"collapse\", \"chaos\", \"corruption\", \"failure\", \"fraud\", \"toxic\",\n",
        "        \"controversy\", \"backlash\", \"boycott\", \"fear\", \"panic\", \"alarm\"\n",
        "    ]\n",
        "\n",
        "    innovation_words = [\n",
        "        \"breakthrough\", \"advancement\", \"innovation\", \"innovative\", \"progress\",\n",
        "        \"future\", \"futuristic\", \"revolutionary\", \"cutting-edge\", \"novel\",\n",
        "        \"development\", \"improvement\", \"evolution\", \"next generation\",\n",
        "        \"transformative\", \"modernization\", \"discovery\", \"pioneering\",\n",
        "        \"groundbreaking\", \"promising\", \"visionary\", \"trendsetting\",\n",
        "        \"game-changing\", \"paradigm shift\", \"progressive\", \"emerging\",\n",
        "        \"enhancement\", \"upgrading\", \"state-of-the-art\"\n",
        "    ]\n",
        "\n",
        "    c1_sample = cluster_df.sample(n=min(200, len(cluster_df)), random_state=42)\n",
        "    c2_sample = cluster_2_df.sample(n=min(200, len(cluster_2_df)), random_state=42)\n",
        "\n",
        "    cluster_1_text = ' '.join(c1_sample['text'].astype(str)).lower()\n",
        "    cluster_2_text = ' '.join(c2_sample['text'].astype(str)).lower()\n",
        "\n",
        "    c1_risk_score = sum(cluster_1_text.count(word) for word in surveillance_words)\n",
        "    c1_innovation_score = sum(cluster_1_text.count(word) for word in innovation_words)\n",
        "\n",
        "    c2_risk_score = sum(cluster_2_text.count(word) for word in surveillance_words)\n",
        "    c2_innovation_score = sum(cluster_2_text.count(word) for word in innovation_words)\n",
        "\n",
        "    c1_words = len(cluster_1_text.split())\n",
        "    c2_words = len(cluster_2_text.split())\n",
        "\n",
        "    comparison_data = {\n",
        "        'Cluster': ['Surveillance (C1)', 'Emerging Tech (C2)'],\n",
        "        'Risk Language': [c1_risk_score/c1_words*1000 if c1_words > 0 else 0,\n",
        "                         c2_risk_score/c2_words*1000 if c2_words > 0 else 0],\n",
        "        'Innovation Language': [c1_innovation_score/c1_words*1000 if c1_words > 0 else 0,\n",
        "                               c2_innovation_score/c2_words*1000 if c2_words > 0 else 0]\n",
        "    }\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "    x = range(len(comparison_df))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar([i - width/2 for i in x], comparison_df['Risk Language'],\n",
        "            width, label='Risk Language', color='red', alpha=0.7)\n",
        "    plt.bar([i + width/2 for i in x], comparison_df['Innovation Language'],\n",
        "            width, label='Innovation Language', color='blue', alpha=0.7)\n",
        "\n",
        "    plt.xlabel('Coverage Type')\n",
        "    plt.ylabel('Word Frequency (per 1000 words)')\n",
        "    plt.title(\"Wired's Editorial Strategy: Risk vs Innovation Discourse\")\n",
        "    plt.xticks(x, comparison_df['Cluster'])\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Wired's Editorial Language Patterns:\")\n",
        "    for i, row in comparison_df.iterrows():\n",
        "        print(f\"{row['Cluster']}:\")\n",
        "        print(f\"  Risk language intensity: {row['Risk Language']:.2f} per 1000 words\")\n",
        "        print(f\"  Innovation language intensity: {row['Innovation Language']:.2f} per 1000 words\")\n",
        "        ratio = row['Innovation Language'] / row['Risk Language'] if row['Risk Language'] > 0 else float('inf')\n",
        "        print(f\"  Innovation/Risk ratio: {ratio:.2f}\")\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "print(\"Starting full analysis of Cluster 2...\")\n",
        "cluster_2_df, cluster_2_analysis = analyze_cluster_2_complete()\n",
        "\n",
        "print(\"\\n comparing Cluster 1 vs Cluster 2...\")\n",
        "discourse_comparison = compare_cluster_1_vs_2_discourse(cluster_df, cluster_2_df)\n"
      ],
      "metadata": {
        "id": "-NWKYGWHoY8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster 2 Analysis\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "from datetime import datetime\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Reload data and model\n",
        "OUT_DIR = \"/content/drive/MyDrive/wired_project\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/wired_project/wired_2014_2024_cleaned.csv\"\n",
        "\n",
        "topic_model = BERTopic.load(f\"{OUT_DIR}/bertopic_final_model_v4_stopwords\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs = df[\"text\"].dropna().astype(str).tolist()\n",
        "\n",
        "print(\"Model and data reloaded, number of documents:\", len(docs))\n",
        "\n",
        "# Cluster 1 data for comparison\n",
        "cluster_1_topics = [7, 12, 23, 26, 28, 32, 33, 36, 52, 55, 62, 63]\n",
        "doc_info = topic_model.get_document_info(docs)\n",
        "cluster_df = doc_info[doc_info[\"Topic\"].isin(cluster_1_topics)].copy()\n",
        "cluster_df[\"text\"] = df.loc[cluster_df.index, \"text\"]\n",
        "cluster_df[\"year\"] = pd.to_datetime(df.loc[cluster_df.index, \"date\"]).dt.year\n",
        "\n",
        "print(\"Cluster 1 data prepared, documents:\", len(cluster_df))\n",
        "\n",
        "def analyze_cluster_2_complete():\n",
        "    \"\"\"Complete analysis of Cluster 2: Emerging Tech and Entertainment\"\"\"\n",
        "\n",
        "    print(\"Cluster 2 Analysis: Emerging Tech and Entertainment\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    cluster_2_topics = [0, 1, 8, 17, 24, 42, 44, 49]\n",
        "    cluster_2_df = doc_info[doc_info[\"Topic\"].isin(cluster_2_topics)].copy()\n",
        "    cluster_2_df[\"text\"] = df.loc[cluster_2_df.index, \"text\"]\n",
        "    cluster_2_df[\"year\"] = pd.to_datetime(df.loc[cluster_2_df.index, \"date\"]).dt.year\n",
        "\n",
        "    print(f\"Total documents: {len(cluster_2_df)}\")\n",
        "    print(f\"Number of topics: {len(cluster_2_topics)}\")\n",
        "    print(f\"Time span: {cluster_2_df['year'].min()} - {cluster_2_df['year'].max()}\")\n",
        "\n",
        "    print(f\"\\nTOPIC BREAKDOWN:\")\n",
        "    topic_analysis = {}\n",
        "\n",
        "    topic_descriptions = {\n",
        "        0: \"General Tech/Mixed Content\",\n",
        "        1: \"VR Gaming/Nintendo/Xbox\",\n",
        "        8: \"Drones/Aviation Technology\",\n",
        "        17: \"Star Wars/Entertainment Media\",\n",
        "        24: \"Camera/Photography Technology\",\n",
        "        42: \"AI Games/AlphaGo/DeepMind\",\n",
        "        44: \"Microsoft/Windows/Operating Systems\",\n",
        "        49: \"Pokemon Go/Mobile Gaming\"\n",
        "    }\n",
        "\n",
        "    for topic_id in cluster_2_topics:\n",
        "        topic_docs = cluster_2_df[cluster_2_df['Topic'] == topic_id]\n",
        "        topic_keywords = topic_model.get_topic(topic_id)\n",
        "\n",
        "        topic_analysis[topic_id] = {\n",
        "            'doc_count': len(topic_docs),\n",
        "            'keywords': [word for word, score in topic_keywords[:5]],\n",
        "            'description': topic_descriptions.get(topic_id, \"Unknown\"),\n",
        "            'peak_year': topic_docs.groupby('year').size().idxmax() if len(topic_docs) > 0 else None\n",
        "        }\n",
        "\n",
        "        keywords_str = ', '.join(topic_analysis[topic_id]['keywords'])\n",
        "        print(f\"Topic {topic_id}: {len(topic_docs)} docs - {topic_analysis[topic_id]['description']}\")\n",
        "        print(f\"  Keywords: {keywords_str}\")\n",
        "        if topic_analysis[topic_id]['peak_year']:\n",
        "            print(f\"  Peak year: {topic_analysis[topic_id]['peak_year']}\")\n",
        "\n",
        "    yearly_counts = cluster_2_df.groupby('year').size().reset_index(name='count')\n",
        "    peak_years = yearly_counts.nlargest(3, 'count')\n",
        "\n",
        "    print(f\"\\nTEMPORAL PATTERNS:\")\n",
        "    print(\"Peak years:\")\n",
        "    for _, row in peak_years.iterrows():\n",
        "        print(f\"  {row['year']}: {row['count']} articles\")\n",
        "\n",
        "    print(f\"\\nCOMPARISON WITH CLUSTER 1:\")\n",
        "    cluster_1_total = len(cluster_df)\n",
        "    cluster_2_total = len(cluster_2_df)\n",
        "    print(f\"Cluster 1 (Surveillance): {cluster_1_total} documents\")\n",
        "    print(f\"Cluster 2 (Emerging Tech): {cluster_2_total} documents\")\n",
        "    print(f\"Coverage ratio (C2/C1): {cluster_2_total/cluster_1_total:.2f}\")\n",
        "\n",
        "    create_cluster_2_visualizations(cluster_2_df, yearly_counts, topic_analysis)\n",
        "\n",
        "    return cluster_2_df, topic_analysis\n",
        "\n",
        "def create_cluster_2_visualizations(cluster_2_df, yearly_counts, topic_analysis):\n",
        "    \"\"\"Visualizations for Cluster 2\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # 1. Yearly trend\n",
        "    axes[0, 0].plot(yearly_counts['year'], yearly_counts['count'],\n",
        "                    marker='o', linewidth=2, markersize=6, color='green')\n",
        "\n",
        "    tech_events = {2016: 'AlphaGo vs Lee', 2020: 'COVID-19'}\n",
        "    for year, event in tech_events.items():\n",
        "        if year in yearly_counts['year'].values:\n",
        "            count = yearly_counts[yearly_counts['year'] == year]['count'].iloc[0]\n",
        "            axes[0, 0].annotate(f'{event}',\n",
        "                               xy=(year, count), xytext=(year, count+15),\n",
        "                               arrowprops=dict(arrowstyle='->', color='red'),\n",
        "                               ha='center', fontsize=8)\n",
        "\n",
        "    axes[0, 0].set_title('Cluster 2: Emerging Tech Coverage Over Time')\n",
        "    axes[0, 0].set_xlabel('Year')\n",
        "    axes[0, 0].set_ylabel('Number of Articles')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 2. Topic distribution (excluding Topic 0)\n",
        "    filtered_topics = {tid: analysis for tid, analysis in topic_analysis.items() if tid != 0}\n",
        "    topic_counts = {tid: analysis['doc_count'] for tid, analysis in filtered_topics.items()}\n",
        "    sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    topic_ids, counts = zip(*sorted_topics)\n",
        "\n",
        "    axes[0, 1].bar(range(len(topic_ids)), counts, color='lightgreen')\n",
        "    axes[0, 1].set_title('Topic Distribution (Focused Technologies)')\n",
        "    axes[0, 1].set_xlabel('Topic ID')\n",
        "    axes[0, 1].set_ylabel('Document Count')\n",
        "    axes[0, 1].set_xticks(range(len(topic_ids)))\n",
        "    axes[0, 1].set_xticklabels([f'T{tid}' for tid in topic_ids], rotation=45)\n",
        "\n",
        "    # 3. Technology category pie chart (excluding Topic 0)\n",
        "    tech_categories = {\n",
        "        'Gaming/VR': [1, 49],\n",
        "        'Hardware/Systems': [8, 24, 44],\n",
        "        'Entertainment': [17],\n",
        "        'AI/ML': [42]\n",
        "    }\n",
        "\n",
        "    category_counts = {}\n",
        "    for category, topic_list in tech_categories.items():\n",
        "        count = sum(topic_analysis[tid]['doc_count'] for tid in topic_list if tid in topic_analysis)\n",
        "        category_counts[category] = count\n",
        "\n",
        "    categories, counts = zip(*category_counts.items())\n",
        "    colors = ['lightblue', 'orange', 'lightgreen', 'red']\n",
        "    axes[1, 0].pie(counts, labels=categories, autopct='%1.1f%%',\n",
        "                   startangle=90, colors=colors)\n",
        "    axes[1, 0].set_title('Technology Category Distribution\\n(Excluding General Content)')\n",
        "\n",
        "    # 4. Top 3 topics evolution (excluding Topic 0)\n",
        "    top_3_topics = [tid for tid, _ in sorted_topics[:3]]\n",
        "    for topic_id in top_3_topics:\n",
        "        topic_docs = cluster_2_df[cluster_2_df['Topic'] == topic_id]\n",
        "        if len(topic_docs) > 0:\n",
        "            topic_yearly = topic_docs.groupby('year').size()\n",
        "            axes[1, 1].plot(topic_yearly.index, topic_yearly.values,\n",
        "                           marker='o', label=f'Topic {topic_id}', linewidth=2)\n",
        "\n",
        "    axes[1, 1].set_title('Top Focused Topics Evolution')\n",
        "    axes[1, 1].set_xlabel('Year')\n",
        "    axes[1, 1].set_ylabel('Articles')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"\\nFocused Technology Distribution:\")\n",
        "    total_focused = sum(counts)\n",
        "    for category, count in zip(categories, counts):\n",
        "        print(f\"  {category}: {count} docs ({count/total_focused*100:.1f}%)\")\n",
        "\n",
        "def compare_cluster_1_vs_2_discourse(cluster_1_df, cluster_2_df):\n",
        "    \"\"\"Compare discourse features of Cluster 1 vs Cluster 2\"\"\"\n",
        "\n",
        "    print(\"\\nCOMPARATIVE DISCOURSE ANALYSIS: Surveillance vs Emerging Tech\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    surveillance_words = [\n",
        "        \"threat\", \"risk\", \"concern\", \"danger\", \"security\", \"privacy\",\n",
        "        \"surveillance\", \"breach\", \"data breach\", \"data leak\", \"hack\",\n",
        "        \"hacking\", \"cyberattack\", \"cybersecurity\", \"ransomware\", \"malware\",\n",
        "        \"virus\", \"spyware\", \"trojan\", \"exposed\", \"unauthorized\",\n",
        "        \"monitoring\", \"tracking\", \"spying\", \"espionage\", \"censorship\",\n",
        "        \"manipulation\", \"propaganda\", \"misinformation\", \"disinformation\",\n",
        "        \"deepfake\", \"fake news\", \"bias\", \"algorithmic bias\", \"black box\",\n",
        "        \"opacity\", \"unethical\", \"unsafe\", \"harmful\", \"catastrophe\",\n",
        "        \"collapse\", \"chaos\", \"corruption\", \"failure\", \"fraud\", \"toxic\",\n",
        "        \"controversy\", \"backlash\", \"boycott\", \"fear\", \"panic\", \"alarm\"\n",
        "    ]\n",
        "\n",
        "    innovation_words = [\n",
        "        \"breakthrough\", \"advancement\", \"innovation\", \"innovative\", \"progress\",\n",
        "        \"future\", \"futuristic\", \"revolutionary\", \"cutting-edge\", \"novel\",\n",
        "        \"development\", \"improvement\", \"evolution\", \"next generation\",\n",
        "        \"transformative\", \"modernization\", \"discovery\", \"pioneering\",\n",
        "        \"groundbreaking\", \"promising\", \"visionary\", \"trendsetting\",\n",
        "        \"game-changing\", \"paradigm shift\", \"progressive\", \"emerging\",\n",
        "        \"enhancement\", \"upgrading\", \"state-of-the-art\"\n",
        "    ]\n",
        "\n",
        "    c1_sample = cluster_df.sample(n=min(200, len(cluster_df)), random_state=42)\n",
        "    c2_sample = cluster_2_df.sample(n=min(200, len(cluster_2_df)), random_state=42)\n",
        "\n",
        "    cluster_1_text = ' '.join(c1_sample['text'].astype(str)).lower()\n",
        "    cluster_2_text = ' '.join(c2_sample['text'].astype(str)).lower()\n",
        "\n",
        "    c1_risk_score = sum(cluster_1_text.count(word) for word in surveillance_words)\n",
        "    c1_innovation_score = sum(cluster_1_text.count(word) for word in innovation_words)\n",
        "\n",
        "    c2_risk_score = sum(cluster_2_text.count(word) for word in surveillance_words)\n",
        "    c2_innovation_score = sum(cluster_2_text.count(word) for word in innovation_words)\n",
        "\n",
        "    c1_words = len(cluster_1_text.split())\n",
        "    c2_words = len(cluster_2_text.split())\n",
        "\n",
        "    comparison_data = {\n",
        "        'Cluster': ['Surveillance (C1)', 'Emerging Tech (C2)'],\n",
        "        'Risk Language': [c1_risk_score/c1_words*1000 if c1_words > 0 else 0,\n",
        "                         c2_risk_score/c2_words*1000 if c2_words > 0 else 0],\n",
        "        'Innovation Language': [c1_innovation_score/c1_words*1000 if c1_words > 0 else 0,\n",
        "                               c2_innovation_score/c2_words*1000 if c2_words > 0 else 0]\n",
        "    }\n",
        "\n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "    x = range(len(comparison_df))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.bar([i - width/2 for i in x], comparison_df['Risk Language'],\n",
        "            width, label='Risk Language', color='red', alpha=0.7)\n",
        "    plt.bar([i + width/2 for i in x], comparison_df['Innovation Language'],\n",
        "            width, label='Innovation Language', color='blue', alpha=0.7)\n",
        "\n",
        "    plt.xlabel('Coverage Type')\n",
        "    plt.ylabel('Word Frequency (per 1000 words)')\n",
        "    plt.title(\"Wired's Editorial Strategy: Risk vs Innovation Discourse\")\n",
        "    plt.xticks(x, comparison_df['Cluster'])\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Wired's Editorial Language Patterns:\")\n",
        "    for i, row in comparison_df.iterrows():\n",
        "        print(f\"{row['Cluster']}:\")\n",
        "        print(f\"  Risk language intensity: {row['Risk Language']:.2f} per 1000 words\")\n",
        "        print(f\"  Innovation language intensity: {row['Innovation Language']:.2f} per 1000 words\")\n",
        "        ratio = row['Innovation Language'] / row['Risk Language'] if row['Risk Language'] > 0 else float('inf')\n",
        "        print(f\"  Innovation/Risk ratio: {ratio:.2f}\")\n",
        "\n",
        "    return comparison_df\n",
        "\n",
        "print(\"Starting full analysis of Cluster 2...\")\n",
        "cluster_2_df, cluster_2_analysis = analyze_cluster_2_complete()\n",
        "\n",
        "print(\"\\n comparing Cluster 1 vs Cluster 2...\")\n",
        "discourse_comparison = compare_cluster_1_vs_2_discourse(cluster_df, cluster_2_df)\n"
      ],
      "metadata": {
        "id": "0NhZDJOdsz7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from bertopic import BERTopic\n",
        "\n",
        "# Reload data and model\n",
        "OUT_DIR = \"/content/drive/MyDrive/wired_project\"\n",
        "CSV_PATH = \"/content/drive/MyDrive/wired_project/wired_2014_2024_cleaned.csv\"\n",
        "\n",
        "topic_model = BERTopic.load(f\"{OUT_DIR}/bertopic_final_model_v4_stopwords\")\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "docs = df[\"text\"].dropna().astype(str).tolist()\n",
        "doc_info = topic_model.get_document_info(docs)\n",
        "\n",
        "print(\"Data reloaded. Document count:\", len(docs))\n",
        "\n",
        "\n",
        "def analyze_cluster_0_consumer_tech_risks():\n",
        "    \"\"\"Cluster 0: Consumer Tech Risk Analysis\"\"\"\n",
        "\n",
        "    print(\"Cluster 0 Analysis: Consumer Tech Risks\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Topics in cluster 0\n",
        "    cluster_0_topics = [4, 13, 21, 61]\n",
        "\n",
        "    cluster_0_df = doc_info[doc_info[\"Topic\"].isin(cluster_0_topics)].copy()\n",
        "    cluster_0_df[\"text\"] = df.loc[cluster_0_df.index, \"text\"]\n",
        "    cluster_0_df[\"year\"] = pd.to_datetime(df.loc[cluster_0_df.index, \"date\"]).dt.year\n",
        "\n",
        "    print(f\"Total documents: {len(cluster_0_df)}\")\n",
        "    print(f\"Number of topics: {len(cluster_0_topics)}\")\n",
        "\n",
        "    # Per-topic analysis\n",
        "    topic_analysis = {}\n",
        "    for topic_id in cluster_0_topics:\n",
        "        topic_docs = cluster_0_df[cluster_0_df['Topic'] == topic_id]\n",
        "        topic_keywords = topic_model.get_topic(topic_id)\n",
        "\n",
        "        if len(topic_docs) > 0:\n",
        "            topic_analysis[topic_id] = {\n",
        "                'doc_count': len(topic_docs),\n",
        "                'keywords': topic_keywords[:10],\n",
        "                'peak_year': topic_docs.groupby('year').size().idxmax()\n",
        "            }\n",
        "\n",
        "            print(f\"\\nTopic {topic_id}: {len(topic_docs)} documents\")\n",
        "            print(f\"  Peak year: {topic_analysis[topic_id]['peak_year']}\")\n",
        "            top_5_keywords = [word for word, score in topic_keywords[:5]]\n",
        "            print(f\"  Top keywords: {', '.join(top_5_keywords)}\")\n",
        "\n",
        "    # Create visualizations\n",
        "    create_topic_keywords_visualization(topic_analysis)\n",
        "    create_cluster_0_risk_analysis(cluster_0_df, topic_analysis)\n",
        "\n",
        "    return cluster_0_df, topic_analysis\n",
        "\n",
        "\n",
        "def create_topic_keywords_visualization(topic_analysis):\n",
        "    \"\"\"Create visualization of topic keywords\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    colors = ['lightcoral', 'lightblue', 'lightgreen', 'gold']\n",
        "    topic_ids = [4, 13, 21, 61]\n",
        "\n",
        "    for i, topic_id in enumerate(topic_ids):\n",
        "        if topic_id in topic_analysis:\n",
        "            keywords_data = topic_analysis[topic_id]['keywords']\n",
        "            words = [word for word, score in keywords_data]\n",
        "            scores = [score for word, score in keywords_data]\n",
        "\n",
        "            y_pos = range(len(words))\n",
        "            bars = axes[i].barh(y_pos, scores, color=colors[i], alpha=0.8)\n",
        "\n",
        "            axes[i].set_yticks(y_pos)\n",
        "            axes[i].set_yticklabels(words)\n",
        "            axes[i].invert_yaxis()\n",
        "            axes[i].set_xlabel('Topic Weight')\n",
        "            axes[i].set_title(f'Topic {topic_id} ({topic_analysis[topic_id][\"doc_count\"]} docs)')\n",
        "\n",
        "            for j, (bar, score) in enumerate(zip(bars, scores)):\n",
        "                axes[i].text(\n",
        "                    bar.get_width() + max(scores) * 0.02,\n",
        "                    bar.get_y() + bar.get_height() / 2,\n",
        "                    f'{score:.3f}',\n",
        "                    ha='left',\n",
        "                    va='center',\n",
        "                    fontsize=8\n",
        "                )\n",
        "\n",
        "            axes[i].set_xlim(0, max(scores) * 1.3)\n",
        "            axes[i].grid(axis='x', alpha=0.3)\n",
        "        else:\n",
        "            axes[i].text(\n",
        "                0.5, 0.5, f'Topic {topic_id}\\nNo data',\n",
        "                transform=axes[i].transAxes, ha='center', va='center'\n",
        "            )\n",
        "\n",
        "    plt.suptitle('Cluster 0: Consumer Tech Risk Topics - Keyword Distribution')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def create_cluster_0_risk_analysis(cluster_df, topic_analysis):\n",
        "    \"\"\"Create risk analysis visualizations\"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # 1. Topic document distribution\n",
        "    topic_ids = list(topic_analysis.keys())\n",
        "    doc_counts = [topic_analysis[tid]['doc_count'] for tid in topic_ids]\n",
        "\n",
        "    bars = axes[0, 0].bar(\n",
        "        range(len(topic_ids)),\n",
        "        doc_counts,\n",
        "        color=['lightcoral', 'lightblue', 'lightgreen', 'gold']\n",
        "    )\n",
        "    axes[0, 0].set_title('Topic Distribution')\n",
        "    axes[0, 0].set_xlabel('Topic ID')\n",
        "    axes[0, 0].set_ylabel('Document Count')\n",
        "    axes[0, 0].set_xticks(range(len(topic_ids)))\n",
        "    axes[0, 0].set_xticklabels([f'T{tid}' for tid in topic_ids])\n",
        "\n",
        "    for bar, count in zip(bars, doc_counts):\n",
        "        axes[0, 0].text(\n",
        "            bar.get_x() + bar.get_width() / 2,\n",
        "            bar.get_height() + max(doc_counts) * 0.01,\n",
        "            str(count),\n",
        "            ha='center',\n",
        "            va='bottom'\n",
        "        )\n",
        "\n",
        "    # 2. Yearly trend\n",
        "    yearly_counts = cluster_df.groupby('year').size().reset_index(name='count')\n",
        "    axes[0, 1].plot(\n",
        "        yearly_counts['year'],\n",
        "        yearly_counts['count'],\n",
        "        marker='o',\n",
        "        linewidth=2,\n",
        "        color='red'\n",
        "    )\n",
        "    axes[0, 1].set_title('Coverage Over Time')\n",
        "    axes[0, 1].set_xlabel('Year')\n",
        "    axes[0, 1].set_ylabel('Articles')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    # 3. Topic trends over time\n",
        "    for topic_id in topic_ids:\n",
        "        topic_docs = cluster_df[cluster_df['Topic'] == topic_id]\n",
        "        if len(topic_docs) > 0:\n",
        "            topic_yearly = topic_docs.groupby('year').size()\n",
        "            axes[1, 0].plot(\n",
        "                topic_yearly.index,\n",
        "                topic_yearly.values,\n",
        "                marker='o',\n",
        "                label=f'Topic {topic_id}'\n",
        "            )\n",
        "\n",
        "    axes[1, 0].set_title('Individual Topic Trends')\n",
        "    axes[1, 0].set_xlabel('Year')\n",
        "    axes[1, 0].set_ylabel('Articles')\n",
        "    axes[1, 0].legend()\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # 4. Relative importance pie chart\n",
        "    axes[1, 1].pie(\n",
        "        doc_counts,\n",
        "        labels=[f'Topic {tid}' for tid in topic_ids],\n",
        "        autopct='%1.1f%%',\n",
        "        colors=['lightcoral', 'lightblue', 'lightgreen', 'gold']\n",
        "    )\n",
        "    axes[1, 1].set_title('Relative Topic Importance')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Run analysis\n",
        "cluster_0_df, cluster_0_analysis = analyze_cluster_0_consumer_tech_risks()"
      ],
      "metadata": {
        "id": "c5xLJw2DG5Zu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster 3 Analysis\n",
        "\n",
        "def analyze_cluster_3_corporate_industry_risks():\n",
        "    \"\"\"Cluster 3: Multi-dimensional Analysis of Corporate and Industry Risks\"\"\"\n",
        "\n",
        "    print(\"Cluster 3 Analysis: Corporate and Industry Risk Coverage\")\n",
        "    print(\"=\" * 65)\n",
        "\n",
        "    # Topics in cluster 3\n",
        "    cluster_3_topics = [6, 9, 10, 16, 18, 19, 20, 29, 30, 31, 35, 39, 40, 41, 43, 50, 60]\n",
        "\n",
        "    cluster_3_df = doc_info[doc_info[\"Topic\"].isin(cluster_3_topics)].copy()\n",
        "    cluster_3_df[\"text\"] = df.loc[cluster_3_df.index, \"text\"]\n",
        "    cluster_3_df[\"year\"] = pd.to_datetime(df.loc[cluster_3_df.index, \"date\"]).dt.year\n",
        "    cluster_3_df[\"month\"] = pd.to_datetime(df.loc[cluster_3_df.index, \"date\"]).dt.month\n",
        "\n",
        "    print(f\"Total documents: {len(cluster_3_df)}\")\n",
        "    print(f\"Topics analyzed: {len(cluster_3_topics)}\")\n",
        "\n",
        "    # Analyses\n",
        "    editorial_stance_analysis = analyze_wired_editorial_stance_shift(cluster_3_df)\n",
        "    coverage_depth_analysis = analyze_coverage_depth_patterns(cluster_3_df)\n",
        "    geographic_risk_analysis = analyze_geographic_risk_distribution(cluster_3_df)\n",
        "\n",
        "    # Visualizations\n",
        "    create_cluster_3_diverse_visualizations(\n",
        "        cluster_3_df,\n",
        "        editorial_stance_analysis,\n",
        "        coverage_depth_analysis,\n",
        "        geographic_risk_analysis\n",
        "    )\n",
        "\n",
        "    return cluster_3_df\n",
        "\n",
        "\n",
        "def analyze_wired_editorial_stance_shift(cluster_df):\n",
        "    \"\"\"Analyze Wired's editorial stance shift on corporate tech\"\"\"\n",
        "\n",
        "    critical_language = [\n",
        "        \"problematic\", \"concerning\", \"questionable\", \"controversial\", \"criticism\",\n",
        "        \"backlash\", \"flawed\", \"biased\", \"unethical\", \"unfair\", \"dangerous\",\n",
        "        \"harmful\", \"shortcomings\", \"limitations\", \"opposition\", \"challenge\",\n",
        "        \"resistance\", \"disputed\", \"skeptical\", \"negative\", \"troubling\",\n",
        "        \"scandal\", \"violation\", \"abuse\", \"failure\", \"collapse\", \"corruption\",\n",
        "        \"misleading\", \"inaccurate\", \"doubtful\", \"critical\", \"condemned\"\n",
        "    ]\n",
        "\n",
        "    neutral_language = [\n",
        "        \"analysis\", \"examination\", \"assessment\", \"evaluation\", \"review\",\n",
        "        \"overview\", \"report\", \"discussion\", \"description\", \"explanation\",\n",
        "        \"study\", \"investigation\", \"comparison\", \"account\", \"summary\",\n",
        "        \"interpretation\", \"findings\", \"observations\", \"consideration\",\n",
        "        \"neutral\", \"outline\", \"exploration\", \"presentation\", \"profiling\"\n",
        "    ]\n",
        "\n",
        "    supportive_language = [\n",
        "        \"innovative\", \"pioneering\", \"groundbreaking\", \"impressive\", \"successful\",\n",
        "        \"leading\", \"promising\", \"cutting-edge\", \"revolutionary\", \"progressive\",\n",
        "        \"remarkable\", \"visionary\", \"transformative\", \"breakthrough\", \"extraordinary\",\n",
        "        \"outstanding\", \"excellent\", \"superior\", \"notable\", \"celebrated\",\n",
        "        \"influential\", \"inspiring\", \"acclaimed\", \"renowned\", \"commendable\",\n",
        "        \"trailblazing\", \"advanced\", \"state-of-the-art\", \"game-changing\", \"exceptional\"\n",
        "    ]\n",
        "\n",
        "    stance_evolution = {}\n",
        "\n",
        "    for year in sorted(cluster_df['year'].unique()):\n",
        "        year_sample = cluster_df[cluster_df['year'] == year].sample(\n",
        "            n=min(30, len(cluster_df[cluster_df['year'] == year])),\n",
        "            random_state=42\n",
        "        )\n",
        "        year_text = ' '.join(year_sample['text']).lower()\n",
        "        total_words = len(year_text.split())\n",
        "\n",
        "        critical_score = sum(year_text.count(word) for word in critical_language)\n",
        "        neutral_score = sum(year_text.count(word) for word in neutral_language)\n",
        "        supportive_score = sum(year_text.count(word) for word in supportive_language)\n",
        "\n",
        "        total_stance = critical_score + neutral_score + supportive_score\n",
        "\n",
        "        if total_stance > 0:\n",
        "            stance_evolution[year] = {\n",
        "                'critical_ratio': critical_score / total_stance,\n",
        "                'neutral_ratio': neutral_score / total_stance,\n",
        "                'supportive_ratio': supportive_score / total_stance,\n",
        "                'stance_intensity': total_stance / total_words * 1000 if total_words > 0 else 0\n",
        "            }\n",
        "\n",
        "    print(\"WIRED'S EDITORIAL STANCE EVOLUTION:\")\n",
        "    recent_critical = stance_evolution[2024]['critical_ratio'] if 2024 in stance_evolution else 0\n",
        "    early_critical = stance_evolution[2014]['critical_ratio'] if 2014 in stance_evolution else 0\n",
        "\n",
        "    trend = \"more critical\" if recent_critical > early_critical else \"less critical\"\n",
        "    print(f\"  Editorial trend: {trend} of corporate tech\")\n",
        "    print(f\"  2014 critical stance: {early_critical:.1%}\")\n",
        "    print(f\"  2024 critical stance: {recent_critical:.1%}\")\n",
        "\n",
        "    return stance_evolution\n",
        "\n",
        "\n",
        "def analyze_coverage_depth_patterns(cluster_df):\n",
        "    \"\"\"Analyze patterns of Wired's reporting depth and complexity\"\"\"\n",
        "\n",
        "    depth_indicators = {\n",
        "        'investigative': ['investigation', 'documents reveal', 'sources say', 'obtained by', 'internal'],\n",
        "        'analytical': ['analysis', 'implications', 'consequences', 'broader context', 'underlying'],\n",
        "        'surface': ['announced', 'released', 'launched', 'unveiled', 'introduced']\n",
        "    }\n",
        "\n",
        "    depth_patterns = {}\n",
        "\n",
        "    for year in sorted(cluster_df['year'].unique()):\n",
        "        year_docs = cluster_df[cluster_df['year'] == year]\n",
        "        avg_length = year_docs['text'].apply(len).mean()\n",
        "        year_text = ' '.join(year_docs['text'].head(50)).lower()\n",
        "\n",
        "        investigative_signals = sum(year_text.count(phrase) for phrase in depth_indicators['investigative'])\n",
        "        analytical_signals = sum(year_text.count(phrase) for phrase in depth_indicators['analytical'])\n",
        "        surface_signals = sum(year_text.count(phrase) for phrase in depth_indicators['surface'])\n",
        "\n",
        "        total_signals = investigative_signals + analytical_signals + surface_signals\n",
        "\n",
        "        depth_patterns[year] = {\n",
        "            'avg_article_length': avg_length,\n",
        "            'investigative_ratio': investigative_signals / total_signals if total_signals > 0 else 0,\n",
        "            'analytical_ratio': analytical_signals / total_signals if total_signals > 0 else 0,\n",
        "            'surface_ratio': surface_signals / total_signals if total_signals > 0 else 0\n",
        "        }\n",
        "\n",
        "    print(\"COVERAGE DEPTH ANALYSIS:\")\n",
        "    avg_length_trend = (\n",
        "        (depth_patterns[2024]['avg_article_length'] - depth_patterns[2014]['avg_article_length'])\n",
        "        / depth_patterns[2014]['avg_article_length'] * 100\n",
        "        if 2014 in depth_patterns and 2024 in depth_patterns else 0\n",
        "    )\n",
        "    print(f\"  Article length trend: {avg_length_trend:+.1f}% change from 2014 to 2024\")\n",
        "\n",
        "    return depth_patterns\n",
        "\n",
        "\n",
        "def analyze_geographic_risk_distribution(cluster_df):\n",
        "    \"\"\"Analyze geographic distribution of corporate risk coverage\"\"\"\n",
        "\n",
        "    regions = {\n",
        "        'Silicon Valley': ['silicon valley', 'san francisco', 'palo alto', 'menlo park', 'cupertino'],\n",
        "        'Seattle Tech': ['seattle', 'redmond', 'bellevue'],\n",
        "        'International': ['china', 'europe', 'london', 'berlin', 'tokyo'],\n",
        "        'East Coast': ['new york', 'boston', 'washington dc']\n",
        "    }\n",
        "\n",
        "    geo_distribution = {}\n",
        "\n",
        "    for year in sorted(cluster_df['year'].unique()):\n",
        "        year_text = ' '.join(cluster_df[cluster_df['year'] == year]['text'].head(30)).lower()\n",
        "\n",
        "        year_geo = {}\n",
        "        for region, locations in regions.items():\n",
        "            mentions = sum(year_text.count(location) for location in locations)\n",
        "            year_geo[region] = mentions\n",
        "\n",
        "        geo_distribution[year] = year_geo\n",
        "\n",
        "    print(\"GEOGRAPHIC FOCUS ANALYSIS:\")\n",
        "    sv_dominance = (\n",
        "        sum(geo_distribution[year]['Silicon Valley'] for year in geo_distribution)\n",
        "        / sum(sum(year_data.values()) for year_data in geo_distribution.values())\n",
        "        if geo_distribution else 0\n",
        "    )\n",
        "    print(f\"  Silicon Valley coverage dominance: {sv_dominance:.1%}\")\n",
        "\n",
        "    return geo_distribution\n",
        "\n",
        "\n",
        "def create_cluster_3_diverse_visualizations(cluster_df, stance_data, depth_data, geo_data):\n",
        "    \"\"\"Create diverse visualizations for Cluster 3\"\"\"\n",
        "\n",
        "    fig = plt.figure(figsize=(20, 12))\n",
        "\n",
        "    # 1. Editorial stance ratios\n",
        "    ax1 = plt.subplot(2, 4, 1)\n",
        "    years = list(stance_data.keys())\n",
        "    critical_ratios = [stance_data[year]['critical_ratio'] for year in years]\n",
        "    neutral_ratios = [stance_data[year]['neutral_ratio'] for year in years]\n",
        "    supportive_ratios = [stance_data[year]['supportive_ratio'] for year in years]\n",
        "\n",
        "    ax1.fill_between(years, critical_ratios, color='red', alpha=0.3, label='Critical')\n",
        "    ax1.fill_between(years, neutral_ratios, color='gray', alpha=0.3, label='Neutral')\n",
        "    ax1.fill_between(years, supportive_ratios, color='green', alpha=0.3, label='Supportive')\n",
        "    ax1.set_title(\"Wired's Editorial Stance Evolution\")\n",
        "    ax1.legend()\n",
        "    ax1.set_ylabel('Stance Ratio')\n",
        "\n",
        "    # 2. Coverage depth (pie chart of recent year)\n",
        "    ax2 = plt.subplot(2, 4, 2)\n",
        "    recent_depth = depth_data[2024] if 2024 in depth_data else depth_data[max(depth_data.keys())]\n",
        "\n",
        "    categories = ['Investigative', 'Analytical', 'Surface']\n",
        "    values = [\n",
        "        recent_depth['investigative_ratio'],\n",
        "        recent_depth['analytical_ratio'],\n",
        "        recent_depth['surface_ratio']\n",
        "    ]\n",
        "\n",
        "    ax2.pie(values, labels=categories, autopct='%1.1f%%',\n",
        "            colors=['darkred', 'orange', 'lightblue'])\n",
        "    ax2.set_title('Current Coverage Depth Mix')\n",
        "\n",
        "    # 3. Geographic distribution\n",
        "    ax3 = plt.subplot(2, 4, (3, 4))\n",
        "    geo_years = list(geo_data.keys())\n",
        "\n",
        "    sv_data = [geo_data[year]['Silicon Valley'] for year in geo_years]\n",
        "    seattle_data = [geo_data[year]['Seattle Tech'] for year in geo_years]\n",
        "    intl_data = [geo_data[year]['International'] for year in geo_years]\n",
        "    east_data = [geo_data[year]['East Coast'] for year in geo_years]\n",
        "\n",
        "    ax3.stackplot(\n",
        "        geo_years, sv_data, seattle_data, intl_data, east_data,\n",
        "        labels=['Silicon Valley', 'Seattle Tech', 'International', 'East Coast'],\n",
        "        colors=['gold', 'lightgreen', 'lightcoral', 'lightblue'],\n",
        "        alpha=0.8\n",
        "    )\n",
        "    ax3.set_title('Geographic Focus of Corporate Risk Coverage')\n",
        "    ax3.legend(loc='upper left', fontsize=8)\n",
        "    ax3.set_ylabel('Geographic Mentions')\n",
        "\n",
        "    # 4. Article length trend\n",
        "    ax4 = plt.subplot(2, 4, 5)\n",
        "    depth_years = list(depth_data.keys())\n",
        "    avg_lengths = [depth_data[year]['avg_article_length'] for year in depth_years]\n",
        "\n",
        "    ax4.plot(depth_years, avg_lengths, 'bo-', linewidth=2, markersize=6)\n",
        "    ax4.fill_between(depth_years, avg_lengths, alpha=0.3, color='blue')\n",
        "    ax4.set_title('Article Length Trend')\n",
        "    ax4.set_ylabel('Avg Characters per Article')\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "\n",
        "    # 5. Monthly coverage heatmap\n",
        "    ax5 = plt.subplot(2, 4, 6)\n",
        "    monthly_data = cluster_df.groupby(['year', 'month']).size().unstack(fill_value=0)\n",
        "    if len(monthly_data) > 0:\n",
        "        im = ax5.imshow(monthly_data.values, cmap='YlOrRd', aspect='auto')\n",
        "        ax5.set_title('Monthly Coverage Intensity')\n",
        "        ax5.set_xlabel('Month')\n",
        "        ax5.set_ylabel('Year')\n",
        "        ax5.set_xticks(range(12))\n",
        "        ax5.set_xticklabels(['J', 'F', 'M', 'A', 'M', 'J', 'J', 'A', 'S', 'O', 'N', 'D'])\n",
        "        ax5.set_yticks(range(len(monthly_data.index)))\n",
        "        ax5.set_yticklabels(monthly_data.index)\n",
        "\n",
        "    # 6. Stance intensity vs coverage volume\n",
        "    ax6 = plt.subplot(2, 4, 7)\n",
        "    yearly_counts = cluster_df.groupby('year').size()\n",
        "    stance_intensities = [stance_data[year]['stance_intensity'] for year in years if year in yearly_counts.index]\n",
        "    coverage_volumes = [yearly_counts[year] for year in years if year in yearly_counts.index]\n",
        "\n",
        "    scatter = ax6.scatter(\n",
        "        stance_intensities,\n",
        "        coverage_volumes,\n",
        "        c=[year for year in years if year in yearly_counts.index],\n",
        "        cmap='viridis', s=60, alpha=0.7\n",
        "    )\n",
        "    ax6.set_xlabel('Editorial Stance Intensity')\n",
        "    ax6.set_ylabel('Coverage Volume')\n",
        "    ax6.set_title('Stance vs Volume Correlation')\n",
        "    plt.colorbar(scatter, ax=ax6, label='Year')\n",
        "\n",
        "    # 7. Composite quality index\n",
        "    ax7 = plt.subplot(2, 4, 8)\n",
        "    composite_scores = []\n",
        "    composite_years = []\n",
        "\n",
        "    for year in years:\n",
        "        if year in depth_data and year in yearly_counts.index:\n",
        "            critical_weight = stance_data[year]['critical_ratio'] * 0.4\n",
        "            depth_weight = depth_data[year]['investigative_ratio'] * 0.3\n",
        "            volume_weight = (yearly_counts[year] / yearly_counts.max()) * 0.3\n",
        "\n",
        "            composite = critical_weight + depth_weight + volume_weight\n",
        "            composite_scores.append(composite)\n",
        "            composite_years.append(year)\n",
        "\n",
        "    ax7.bar(composite_years, composite_scores, color='purple', alpha=0.7)\n",
        "    ax7.set_title('Wired Corporate Coverage\\nQuality Index')\n",
        "    ax7.set_ylabel('Composite Score')\n",
        "    ax7.tick_params(axis='x', rotation=45)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Run Cluster 3 analysis\n",
        "cluster_3_df = analyze_cluster_3_corporate_industry_risks()"
      ],
      "metadata": {
        "id": "gKqTWQ0LbdVo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cluster 4\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "def analyze_cluster_4_policy(topic_model, doc_info, df):\n",
        "    \"\"\"Cluster 4: Technology Policy and Communication Systems\"\"\"\n",
        "\n",
        "    # Topics in cluster 4\n",
        "    cluster_4_topics = [14, 18, 56, 64]\n",
        "    cluster_4_df = doc_info[doc_info[\"Topic\"].isin(cluster_4_topics)].copy()\n",
        "    cluster_4_df[\"text\"] = df.loc[cluster_4_df.index, \"text\"]\n",
        "    cluster_4_df[\"year\"] = pd.to_datetime(df.loc[cluster_4_df.index, \"date\"]).dt.year\n",
        "\n",
        "    print(\"Cluster 4: Technology Policy and Communication Systems\")\n",
        "    print(\"=\" * 70)\n",
        "    print(f\"Total documents: {len(cluster_4_df)}\")\n",
        "    print(f\"Topics: {cluster_4_topics}\")\n",
        "\n",
        "    # Keyword distributions\n",
        "    fig, axes = plt.subplots(1, len(cluster_4_topics), figsize=(16, 4))\n",
        "    for i, tid in enumerate(cluster_4_topics):\n",
        "        kws = topic_model.get_topic(tid)[:10]\n",
        "        words, scores = zip(*kws)\n",
        "        axes[i].barh(words[::-1], scores[::-1], color=\"teal\")\n",
        "        axes[i].set_title(f\"Topic {tid}\")\n",
        "    plt.suptitle(\"Technology Policy and Communication Systems - Keyword Distributions\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Temporal trends (overall + per topic)\n",
        "    yearly_counts = cluster_4_df.groupby(\"year\").size()\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.plot(yearly_counts.index, yearly_counts.values, marker=\"o\", color=\"darkred\")\n",
        "    plt.title(\"Technology Policy and Communication Systems - Coverage Over Time\")\n",
        "    plt.xlabel(\"Year\"); plt.ylabel(\"Articles\")\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for tid in cluster_4_topics:\n",
        "        topic_docs = cluster_4_df[cluster_4_df[\"Topic\"] == tid]\n",
        "        if not topic_docs.empty:\n",
        "            topic_yearly = topic_docs.groupby(\"year\").size()\n",
        "            plt.plot(topic_yearly.index, topic_yearly.values, marker=\"o\", label=f\"Topic {tid}\")\n",
        "    plt.title(\"Technology Policy and Communication Systems - Individual Topic Trends\")\n",
        "    plt.xlabel(\"Year\"); plt.ylabel(\"Articles\")\n",
        "    plt.legend(); plt.grid(alpha=0.3)\n",
        "    plt.show()\n",
        "\n",
        "    # Document length distribution\n",
        "    cluster_4_df[\"doc_length\"] = cluster_4_df[\"text\"].str.len()\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    cluster_4_df[\"doc_length\"].hist(bins=30, color=\"orange\", edgecolor=\"black\")\n",
        "    plt.axvline(cluster_4_df[\"doc_length\"].mean(), color=\"red\", linestyle=\"--\",\n",
        "                label=f\"Mean length: {cluster_4_df['doc_length'].mean():.0f}\")\n",
        "    plt.title(\"Document Length Distribution - Technology Policy and Communication Systems\")\n",
        "    plt.xlabel(\"Characters\"); plt.ylabel(\"Frequency\")\n",
        "    plt.legend(); plt.show()\n",
        "\n",
        "    # Top entities (NER)\n",
        "    try:\n",
        "        import spacy\n",
        "        nlp = spacy.load(\"en_core_web_sm\")\n",
        "        texts = cluster_4_df[\"text\"].sample(min(200, len(cluster_4_df)), random_state=42).tolist()\n",
        "        ents = []\n",
        "        for doc in nlp.pipe(texts, disable=[\"parser\", \"tagger\"]):\n",
        "            ents.extend([ent.text for ent in doc.ents if ent.label_ in [\"GPE\", \"ORG\"]])\n",
        "        top_ents = Counter(ents).most_common(15)\n",
        "        ents_df = pd.DataFrame(top_ents, columns=[\"Entity\", \"Count\"])\n",
        "        ents_df.plot(kind=\"barh\", x=\"Entity\", y=\"Count\", figsize=(8, 5), color=\"steelblue\")\n",
        "        plt.title(\"Top Entities in Technology Policy and Communication Systems\")\n",
        "        plt.show()\n",
        "    except:\n",
        "        print(\"spaCy not installed or model not available - skip NER.\")\n",
        "\n",
        "    return cluster_4_df\n",
        "\n",
        "\n",
        "# Run Cluster 4 analysis\n",
        "cluster_4_df = analyze_cluster_4_policy(topic_model, doc_info, df)\n"
      ],
      "metadata": {
        "id": "Kfk4gBs8SRlS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
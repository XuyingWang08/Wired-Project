{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import json\n",
        "import time\n",
        "import hashlib\n",
        "import logging\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "from urllib.parse import urljoin, urlparse\n",
        "\n",
        "import requests\n",
        "import requests_cache\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "from dateutil import parser as dateparser\n",
        "from tqdm import tqdm\n",
        "\n",
        "SITEMAP_INDEX = \"https://www.wired.com/sitemap.xml\"\n",
        "BASE = \"https://www.wired.com\"\n",
        "USER_AGENT = \"Mozilla/5.0 (compatible; WiredRiskStudyBot/1.0; +https://wired.com/)\"\n",
        "\n",
        "START_YEAR = 2014\n",
        "END_YEAR = 2024\n",
        "\n",
        "# polite rate limit between network requests\n",
        "REQUEST_INTERVAL = 1.0\n",
        "\n",
        "# length filter after cleaning\n",
        "MIN_LEN = 300\n",
        "MAX_LEN = 100_000\n",
        "\n",
        "# log setup\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s | %(levelname)s | %(message)s\"\n",
        ")\n",
        "logger = logging.getLogger(\"wired_crawler\")\n",
        "\n",
        "def session_with_cache(cache_path: Path) -> requests.Session:\n",
        "    \"\"\"Create a requests session with transparent caching.\"\"\"\n",
        "    # expire_after=None => persistent cache until manually removed\n",
        "    requests_cache.install_cache(\n",
        "        cache_name=str(cache_path),\n",
        "        backend=\"sqlite\",\n",
        "        expire_after=None,\n",
        "        allowable_methods=(\"GET\", \"HEAD\"),\n",
        "        stale_if_error=True,\n",
        "    )\n",
        "    s = requests.Session()\n",
        "    s.headers.update({\"User-Agent\": USER_AGENT})\n",
        "    return s\n",
        "\n",
        "\n",
        "def sleep_politely(resp):\n",
        "    \"\"\"Sleep only if this was a real network hit (not from cache).\"\"\"\n",
        "    from_cache = getattr(resp, \"from_cache\", False)\n",
        "    if not from_cache:\n",
        "        time.sleep(REQUEST_INTERVAL)\n",
        "\n",
        "\n",
        "def get_soup(session, url, **kwargs) -> BeautifulSoup | None:\n",
        "    try:\n",
        "        resp = session.get(url, timeout=30, **kwargs)\n",
        "        sleep_politely(resp)\n",
        "        if resp.status_code != 200:\n",
        "            return None\n",
        "        return BeautifulSoup(resp.text, \"lxml\")\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Failed to GET {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_iso_date(s: str) -> str | None:\n",
        "    \"\"\"Parse date string into ISO YYYY-MM-DD.\"\"\"\n",
        "    if not s:\n",
        "        return None\n",
        "    try:\n",
        "        dt = dateparser.parse(s)\n",
        "        if not dt:\n",
        "            return None\n",
        "        return dt.date().isoformat()\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "\n",
        "def within_year_range(iso_date: str) -> bool:\n",
        "    if not iso_date or len(iso_date) < 4:\n",
        "        return False\n",
        "    try:\n",
        "        y = int(iso_date[:4])\n",
        "        return (START_YEAR <= y <= END_YEAR)\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "def clean_text(raw_html: str) -> str:\n",
        "    \"\"\"Normalize whitespace, remove URLs, lower-case.\"\"\"\n",
        "    if not raw_html:\n",
        "        return \"\"\n",
        "    text = raw_html\n",
        "\n",
        "    # remove inline URLs\n",
        "    text = re.sub(r\"http[s]?://\\S+\", \" \", text)\n",
        "\n",
        "    # collapse whitespace\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "    # lower-case for modeling consistency\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "\n",
        "def dedup_key(title: str, text: str) -> str:\n",
        "    base = (title or \"\").strip() + \"||\" + (text or \"\").strip()\n",
        "    return hashlib.sha1(base.encode(\"utf-8\", errors=\"ignore\")).hexdigest()\n",
        "\n",
        "\n",
        "# Sitemap Collection\n",
        "\n",
        "def parse_sitemap_xml(session, url: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Parse a sitemap or sitemap index; return list of dicts:\n",
        "        {\"loc\": <url>, \"lastmod\": <iso or None>}\n",
        "    \"\"\"\n",
        "    soup = get_soup(session, url)\n",
        "    if soup is None:\n",
        "        return []\n",
        "\n",
        "    out = []\n",
        "    # sitemapindex\n",
        "    for sm in soup.find_all(\"sitemap\"):\n",
        "        loc = sm.find(\"loc\")\n",
        "        lastmod = sm.find(\"lastmod\")\n",
        "        if loc and loc.text:\n",
        "            out.append({\"loc\": loc.text.strip(), \"lastmod\": (lastmod.text.strip() if lastmod else None)})\n",
        "\n",
        "    # actual URLs\n",
        "    for u in soup.find_all(\"url\"):\n",
        "        loc = u.find(\"loc\")\n",
        "        lastmod = u.find(\"lastmod\")\n",
        "        if loc and loc.text:\n",
        "            out.append({\"loc\": loc.text.strip(), \"lastmod\": (lastmod.text.strip() if lastmod else None)})\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def collect_article_urls_from_sitemaps(session) -> set[str]:\n",
        "    \"\"\"\n",
        "    Traverse sitemap index; collect article URLs (filter to 2014–2024 by URL-year or lastmod).\n",
        "    \"\"\"\n",
        "    logger.info(\"Collecting URLs from sitemap index...\")\n",
        "    urls = set()\n",
        "    queue = [SITEMAP_INDEX]\n",
        "    seen_sitemaps = set()\n",
        "\n",
        "    pbar = tqdm(total=0, desc=\"Sitemaps\", unit=\"smap\")\n",
        "    while queue:\n",
        "        smap = queue.pop()\n",
        "        if smap in seen_sitemaps:\n",
        "            continue\n",
        "        seen_sitemaps.add(smap)\n",
        "\n",
        "        entries = parse_sitemap_xml(session, smap)\n",
        "        pbar.total = pbar.total + 1\n",
        "        pbar.update(1)\n",
        "\n",
        "        # classify entries\n",
        "        for e in entries:\n",
        "            loc = e.get(\"loc\")\n",
        "            if not loc:\n",
        "                continue\n",
        "            if loc.endswith(\".xml\")\n",
        "                queue.append(loc)\n",
        "            else:\n",
        "\n",
        "                year_ok = False\n",
        "                try:\n",
        "                    path = urlparse(loc).path\n",
        "                    m = re.search(r\"/(20\\d{2})/\", path)\n",
        "                    if m:\n",
        "                        y = int(m.group(1))\n",
        "                        if START_YEAR <= y <= END_YEAR:\n",
        "                            year_ok = True\n",
        "                    else:\n",
        "                        iso = parse_iso_date(e.get(\"lastmod\"))\n",
        "                        year_ok = within_year_range(iso) if iso else True\n",
        "                except Exception:\n",
        "                    year_ok = True\n",
        "                if year_ok and loc.startswith(BASE):\n",
        "                    urls.add(loc)\n",
        "\n",
        "    pbar.close()\n",
        "    logger.info(f\"Sitemap URL count (pre-unique): {len(urls)}\")\n",
        "    return urls\n",
        "\n",
        "\n",
        "# Archive Discovery\n",
        "\n",
        "def discover_monthly_archive_urls() -> list[str]:\n",
        "    \"\"\"\n",
        "    Build YYYY/MM directory listing URLs to probe (best-effort; many may redirect).\n",
        "    Example: https://www.wired.com/2016/05/\n",
        "    \"\"\"\n",
        "    urls = []\n",
        "    for y in range(START_YEAR, END_YEAR + 1):\n",
        "        for m in range(1, 13):\n",
        "            urls.append(f\"{BASE}/{y:04d}/{m:02d}/\")\n",
        "    return urls\n",
        "\n",
        "\n",
        "def extract_links_from_archive_page(session, url: str) -> set[str]:\n",
        "    \"\"\"\n",
        "    Parse a year/month page to find article links.\n",
        "    \"\"\"\n",
        "    soup = get_soup(session, url)\n",
        "    if soup is None:\n",
        "        return set()\n",
        "\n",
        "    links = set()\n",
        "\n",
        "    for a in soup.find_all(\"a\", href=True):\n",
        "        href = a[\"href\"]\n",
        "        if href.startswith(\"/\"):\n",
        "            href = urljoin(BASE, href)\n",
        "        if href.startswith(BASE):\n",
        "\n",
        "            if re.search(r\"/(story|article|gallery|video)/\", href) or re.search(r\"/\\d{4}/\\d{2}/\\d{2}/\", href):\n",
        "                links.add(href)\n",
        "    return links\n",
        "\n",
        "\n",
        "def collect_article_urls_from_archives(session) -> set[str]:\n",
        "    logger.info(\"Collecting URLs from year/month archives...\")\n",
        "    urls = set()\n",
        "    for url in tqdm(discover_monthly_archive_urls(), desc=\"Archives\", unit=\"month\"):\n",
        "        found = extract_links_from_archive_page(session, url)\n",
        "        urls.update(found)\n",
        "    logger.info(f\"Archive URL count (pre-unique): {len(urls)}\")\n",
        "    return urls\n",
        "\n",
        "\n",
        "# Article Parsing\n",
        "\n",
        "def extract_from_json_ld(soup: BeautifulSoup) -> dict:\n",
        "    \"\"\"\n",
        "    Try JSON-LD first. Wired often embeds Article schema.\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "    for tag in soup.find_all(\"script\", type=lambda t: t and \"ld+json\" in t):\n",
        "        try:\n",
        "            payload = json.loads(tag.string or \"\")\n",
        "\n",
        "            if isinstance(payload, list):\n",
        "                for item in payload:\n",
        "                    if isinstance(item, dict) and item.get(\"@type\", \"\").lower() in {\"article\", \"newsarticle\", \"blogposting\"}:\n",
        "                        data.setdefault(\"title\", item.get(\"headline\"))\n",
        "                        data.setdefault(\"date\", item.get(\"datePublished\") or item.get(\"dateCreated\"))\n",
        "\n",
        "                        if item.get(\"articleBody\"):\n",
        "                            data.setdefault(\"text\", item.get(\"articleBody\"))\n",
        "            elif isinstance(payload, dict):\n",
        "                if payload.get(\"@type\", \"\").lower() in {\"article\", \"newsarticle\", \"blogposting\"}:\n",
        "                    data.setdefault(\"title\", payload.get(\"headline\"))\n",
        "                    data.setdefault(\"date\", payload.get(\"datePublished\") or payload.get(\"dateCreated\"))\n",
        "                    if payload.get(\"articleBody\"):\n",
        "                        data.setdefault(\"text\", payload.get(\"articleBody\"))\n",
        "        except Exception:\n",
        "            continue\n",
        "    return data\n",
        "\n",
        "\n",
        "def extract_via_selectors(soup: BeautifulSoup) -> dict:\n",
        "    \"\"\"\n",
        "    Fallback extraction using common selectors.\n",
        "    \"\"\"\n",
        "    data = {}\n",
        "\n",
        "    # title candidates\n",
        "    title = None\n",
        "    cand = [\n",
        "        ('meta[property=\"og:title\"]', lambda t: t.get(\"content\")),\n",
        "        (\"h1\", lambda t: t.get_text(\" \", strip=True)),\n",
        "        ('meta[name=\"twitter:title\"]', lambda t: t.get(\"content\")),\n",
        "        (\"title\", lambda t: t.get_text(\" \", strip=True)),\n",
        "    ]\n",
        "    for sel, getter in cand:\n",
        "        el = soup.select_one(sel)\n",
        "        if el:\n",
        "            title = getter(el)\n",
        "            if title:\n",
        "                break\n",
        "    if title:\n",
        "        data[\"title\"] = title\n",
        "\n",
        "    # date candidates\n",
        "    date_str = None\n",
        "    date_sel = [\n",
        "        (\"time\", lambda t: t.get(\"datetime\") or t.get_text(\" \", strip=True)),\n",
        "        ('meta[property=\"article:published_time\"]', lambda t: t.get(\"content\")),\n",
        "        ('meta[name=\"date\"]', lambda t: t.get(\"content\")),\n",
        "        ('meta[name=\"pubdate\"]', lambda t: t.get(\"content\")),\n",
        "    ]\n",
        "    for sel, getter in date_sel:\n",
        "        el = soup.select_one(sel)\n",
        "        if el:\n",
        "            date_str = getter(el)\n",
        "            if date_str:\n",
        "                break\n",
        "    if date_str:\n",
        "        data[\"date\"] = date_str\n",
        "\n",
        "    # body\n",
        "    for bad in soup([\"script\", \"style\", \"noscript\"]):\n",
        "        bad.extract()\n",
        "\n",
        "    # try article tag first\n",
        "    article = soup.find(\"article\")\n",
        "    if article:\n",
        "        paragraphs = [p.get_text(\" \", strip=True) for p in article.find_all([\"p\", \"div\"]) if p.get_text(strip=True)]\n",
        "        body = \"\\n\".join(paragraphs)\n",
        "    else:\n",
        "\n",
        "        paragraphs = [p.get_text(\" \", strip=True) for p in soup.find_all(\"p\") if p.get_text(strip=True)]\n",
        "        body = \"\\n\".join(paragraphs)\n",
        "    if body:\n",
        "        data[\"text\"] = body\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def parse_article(session, url: str) -> dict | None:\n",
        "    soup = get_soup(session, url)\n",
        "    if soup is None:\n",
        "        return None\n",
        "\n",
        "    # Prefer JSON-LD\n",
        "    data = extract_from_json_ld(soup)\n",
        "\n",
        "    # Fallbacks\n",
        "    fallback = extract_via_selectors(soup)\n",
        "    for k, v in fallback.items():\n",
        "        data.setdefault(k, v)\n",
        "\n",
        "    title = (data.get(\"title\") or \"\").strip()\n",
        "    raw_date = (data.get(\"date\") or \"\").strip()\n",
        "    text_raw = (data.get(\"text\") or \"\").strip()\n",
        "\n",
        "    # date normalization\n",
        "    iso_date = parse_iso_date(raw_date)\n",
        "    # if still None, try date guess from URL\n",
        "    if not iso_date:\n",
        "        m = re.search(r\"/(20\\d{2})/(\\d{2})/(\\d{2})/\", urlparse(url).path)\n",
        "        if m:\n",
        "            iso_date = f\"{m.group(1)}-{m.group(2)}-{m.group(3)}\"\n",
        "\n",
        "    if not title or not iso_date or not text_raw:\n",
        "        return None\n",
        "\n",
        "    # cleaning\n",
        "    text_clean = clean_text(text_raw)\n",
        "\n",
        "    # filter by length\n",
        "    if not (MIN_LEN <= len(text_clean) <= MAX_LEN):\n",
        "        return None\n",
        "\n",
        "    # filter by year range\n",
        "    if not within_year_range(iso_date):\n",
        "        return None\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"date\": iso_date,\n",
        "        \"text\": text_clean,\n",
        "        \"url\": url\n",
        "    }\n",
        "\n",
        "\n",
        "# Pipeline\n",
        "\n",
        "def collect_all_urls(session) -> set[str]:\n",
        "    urls = set()\n",
        "\n",
        "    urls |= collect_article_urls_from_sitemaps(session)\n",
        "    urls |= collect_article_urls_from_archives(session)\n",
        "\n",
        "    # only keep wired.com\n",
        "    urls = {u for u in urls if u.startswith(BASE)}\n",
        "\n",
        "\n",
        "    filtered = set()\n",
        "    for u in urls:\n",
        "        path = urlparse(u).path\n",
        "        m = re.search(r\"/(20\\d{2})/\", path)\n",
        "        if m:\n",
        "            y = int(m.group(1))\n",
        "            if START_YEAR <= y <= END_YEAR:\n",
        "                filtered.add(u)\n",
        "        else:\n",
        "            filtered.add(u)\n",
        "    logger.info(f\"Total candidate URLs after coarse filtering: {len(filtered)}\")\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def crawl_and_clean(session, urls: set[str], out_csv: Path):\n",
        "    seen = set()\n",
        "    records = []\n",
        "    dup = 0\n",
        "    kept = 0\n",
        "    skipped = 0\n",
        "\n",
        "    for url in tqdm(sorted(urls), desc=\"Crawling articles\", unit=\"url\"):\n",
        "        art = parse_article(session, url)\n",
        "        if not art:\n",
        "            skipped += 1\n",
        "            continue\n",
        "        key = dedup_key(art[\"title\"], art[\"text\"])\n",
        "        if key in seen:\n",
        "            dup += 1\n",
        "            continue\n",
        "        seen.add(key)\n",
        "        records.append(art)\n",
        "        kept += 1\n",
        "\n",
        "    logger.info(f\"Parsed: kept={kept}, dup_skipped={dup}, invalid/skipped={skipped}\")\n",
        "\n",
        "    if records:\n",
        "        df = pd.DataFrame.from_records(records, columns=[\"title\", \"date\", \"text\", \"url\"])\n",
        "        # final sanity\n",
        "        df = df.dropna(subset=[\"title\", \"date\", \"text\"])\n",
        "        # ensure string types\n",
        "        for col in [\"title\", \"date\", \"text\", \"url\"]:\n",
        "            df[col] = df[col].astype(str)\n",
        "\n",
        "        # save CSV\n",
        "        df.to_csv(out_csv, index=False, quoting=csv.QUOTE_MINIMAL)\n",
        "        logger.info(f\"Saved CSV: {out_csv}  (rows={len(df)})\")\n",
        "    else:\n",
        "        logger.warning(\"No records to save.\")\n",
        "\n",
        "\n",
        "# Main\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description=\"Wired 2014–2024 crawler & cleaner\")\n",
        "    parser.add_argument(\"--out\", type=Path, default=Path(\"wired_2014_2024_cleaned.csv\"),\n",
        "                        help=\"Output CSV path\")\n",
        "    parser.add_argument(\"--cache\", type=Path, default=Path(\"./http_cache\"),\n",
        "                        help=\"HTTP cache directory (requests-cache sqlite)\")\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    args.cache.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    session = session_with_cache(args.cache)\n",
        "\n",
        "    urls = collect_all_urls(session)\n",
        "\n",
        "    crawl_and_clean(session, urls, args.out)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "mtpl30N_5kly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9NTe4zdk5kqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VMN8NaJA5ksv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}